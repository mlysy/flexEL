---
title: "Empirical Likelihood Algorithms"
author: "Shimeng Huang, Martin Lysy"
date: "`r Sys.Date()`"
output:
  bookdown::html_vignette2:
    toc: true
vignette: >
  %\VignetteIndexEntry{Empirical Likelihood Algorithms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: flexEL.bib
---

<!-- <script type="text/x-mathjax-config"> -->
<!-- MathJax.Hub.Config({ -->
<!--   TeX: { equationNumbers: { autoNumber: "AMS" } } -->
<!-- }); -->
<!-- </script> -->

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\EL}{EL}
\DeclareMathOperator{\CEL}{CEL}
\newcommand{\SCEL}{\textrm{SCEL}}
\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\bz}{{\bm{0}}}
\newcommand{\bg}{{\bm{g}}}
\newcommand{\XX}{{\bm{X}}}
\newcommand{\xx}{{\bm{x}}}
\newcommand{\zz}{{\bm{z}}}
\newcommand{\bbe}{{\bm{\beta}}}
\newcommand{\bga}{{\bm{\gamma}}}
\newcommand{\bla}{{\bm{\lambda}}}
\newcommand{\w}{{\omega}}
\newcommand{\bom}{{\bm{\omega}}}
\newcommand{\eps}{{\epsilon}}
\newcommand{\tth}{{\bm{\theta}}}
\newcommand{\E}{\textrm{E}}
\newcommand{\F}{\textrm{F}}
\newcommand{\R}{\mathbb R}
\newcommand{\var}{\textrm{var}}
\newcommand{\iid}{\stackrel {\textrm{iid}}{\sim}}
\newcommand{\N}{\mathcal N}
\newcommand{\ud}{\mathop{}\!\mathrm{d}}
\newcommand{\dth}{\frac{\ud}{\ud\tth}}
\newcommand{\indicator}{\operatorname{\mathbb{1}}}
\newcommand{\indsmooth}{\operatorname{\mathcal{S}}}

# Weighted Empirical Likelihood

The weighted empirical likelihood (EL) is defined as
$$
\log \EL(\tth) = \sum_{i=1}^n q_i \log (\hat \omega_i),
$$
where $\hat \bom = \hat \bom(\tth)$ solves the optimization problem
\begin{equation}
\max_{\bom} \sum_{i=1}^n q_i \log(\omega_i) \qquad \Big\vert \qquad \sum_{i=1}^n \omega_i \cdot \bg_i = \bz, \quad \bom \in \Delta^{n-1},
(\#eq:wel)
\end{equation}
where $\bg_i  = \bg_i(\tth) \in \mathbb R^m$, $q_i > 0$ are the weights, and $\Delta^{n-1} = \{\bom \in \mathbb R^n: \sum_{i=1}^n \omega_i = 1, \omega_i > 0\}$ is the $(n-1)$-dimensional probability simplex.

## Solution

Let $r_i = q_i / \sum_{i=1}^n q_i$.  Then the solution to the weighted EL problem is given by
\begin{equation}
\hat \omega_i = \frac{r_i}{1 - \hat \bla'\bg_i}, 
(\#eq:omegahat)
\end{equation}
where 
\begin{equation}
\begin{aligned}
\hat \bla & = \argmax_{\bla} Q(\bla), \\
Q(\bla) & = \sum_{i=1}^n r_i \log^\star\left(1 - \bla'\bg_i; r_i\right), \\
\log^\star(x; r) & = 
\begin{cases} 
\log(x) & x \ge r \\
- \tfrac 1 2 (x/r)^2 + 2 (x/r) - \tfrac 3 2 + \log r & x < r.
\end{cases}
\end{aligned}
(\#eq:optim)
\end{equation}
The convex optimization problem in $\bla$ can be solved by the Newton-Raphson algorithm.  That is, the Newton-Raphson updates are given by
$$
\bla_{t+1} = \bla_{t} - [Q_{(2)}(\bla_t)]^{-1} Q_{(1)}(\bla_t),
$$
where
\begin{equation}
\begin{aligned}
Q_{(1)}(\bla) = \frac{\ud}{\ud \bla} Q(\bla) & = -\sum_{i=1}^n r_i \log^\star_{(1)}(1 - \bla'\bg_i; r_i) \bg_i, \\
Q_{(2)}(\bla) = \frac{\ud^2}{\ud \bla \ud \bla'} Q(\bla) & = \sum_{i=1}^n r_i \log^\star_{(2)}(1 - \bla' \bg_i; r_i) \bg_i \bg_i',
\end{aligned}
(\#eq:nr)
\end{equation}

## Gradient

The gradient of the the weighted empirical loglikelihood with respect to $\tt$ is
$$
\begin{aligned}
\dth \log \EL(\tth) & = \dth \sum_{i=1}^n q_i \log \hat \omega_i(\tth) \\
& = \sum_{i=1}^n \frac{q_i}{1 - \bla'\bg_i} \cdot \dth \bla(\tth)'\bg_i(\tth) \\
& = \dth \bla(\tth)' \left[\sum_{i=1}^n \frac{q_i \cdot \bg_i(\tth)}{1 - \bla'\bg_i}\right] + Q \sum_{i=1}^n \hat \omega_i(\tth) \cdot \bla(\tth)' \dth \bg_i(\tth),
\end{aligned}
$$
where $Q = \sum_{i=1}^n q_i$.  Since substituting \@ref(eq:omegahat) into \@ref(eq:wel) gives
$$
\sum_{i=1}^n \hat \omega_i \cdot \bg_i = \sum_{i=1}^n \frac{r_i \cdot \bg_i}{1 - \hat \bla'\bg_i} = \bz,
$$
the first term in the gradient disappears, such that we have
\begin{equation}
\dth \log \EL(\tth) = Q \sum_{i=1}^n \hat \omega_i(\tth) \cdot \bla(\tth)' \dth \bg_i(\tth).
(\#eq:gradel)
\end{equation}

# Censored Empirical Likelihood

Consider the location-scale model

$$
y_i = \mu(\xx_i, \tth) + \sigma(\xx_i, \tth) + \eps_i,
$$

where $\xx_i \iid F(\xx)$ and $\eps_i \iid G(\eps)$ is a general residual distribution with $E[\eps] = 0$ and $\var(\eps) = 1$.  Suppose we observe $u_i = \min(y_i, c_i)$ where $c_i$ is the censoring time, and we also observe $\delta_i = \indicator\{y_i \le c_i\}$.  Let $e_i = e_i(\tth) = (u_i - \mu(\xx_i, \tth))/\sigma(\xx_i, \tth)$.  For the moment conditions

$$
E[\bg(\xx, \eps, \tth)] = \bz,
$$

The EL with smoothed right censoring is defined as
\begin{equation}
\log \CEL(\tth) = \log \CEL(\tth, \hat{\bom}) = \sum_{i=1}^n \left[\delta_i \log(\hat\w_i) + (1-\delta_i) \log\left(\sum_{j=1}^n \indsmooth(e_i - e_j; s) \cdot \hat\w_j \right)\right],
\end{equation}

where $\indsmooth(\cdot; s)$ is the smoothed indicator function,

$$
\indsmooth(x; s) = \frac{1}{1 + \exp(s \cdot x)},
$$

and $\hat{\bom} = \hat{\bom}(\tth)$ solves the optimization problem

\begin{equation}
\max_{\bom} \log \CEL(\tth, \bom) \qquad \Big\vert \qquad \sum_{i=1}^n \omega_i \cdot \bg_i = \bz, \quad \bom \in \Delta^{n-1}.
\end{equation}

## EM Algorithm

