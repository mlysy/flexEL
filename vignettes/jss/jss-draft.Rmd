---
title:
  formatted: "\\pkg{flexEL}: A Fast and Flexible Framework for Empirical Likelihood Modeling"
  plain: "flexEL: A Fast and Flexible Framework for Empirical Likelihood Modeling"
author: 
  - name: Shimeng Huang
    affiliation: University of Waterloo
  - name: Martin Lysy
    affiliation: University of Waterloo
    address:
      - 200 University Avenue West
      - Ontario, Canada
    email: \email{mlysy@uwaterloo.ca}
abstract: >
  Here is the abstract.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
date: "`r Sys.Date()`"
documentclass: jss
classoption: article
header-includes:
  - \usepackage{caption}
  - \captionsetup[table]{skip=.1em}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \newcommand{\yy}{\bm y}
  - \newcommand{\uu}{\bm u}
  - \newcommand{\tth}{\bm \theta}
  - \newcommand{\lla}{\bm \lambda}
  - \newcommand{\R}{\mathbb R}
  - \renewcommand{\gg}{\bm g}
  - \newcommand{\w}{\omega}
  - \renewcommand{\l}{\lambda}
  - \renewcommand{\L}{\mathcal{L}}
  - \renewcommand{\E}{\textrm{E}}
  - \newcommand{\EL}{\textrm{EL}}
  - \newcommand{\CEL}{\textrm{CEL}}
  - \newcommand{\str}[1]{{#1}^{\star}}
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \newcommand{\cx}{\bm{{\scriptstyle \mathcal X}}}
output: 
  bookdown::pdf_book:
    toc: false
    template: jss-template.tex
    # keep_tex: true
    # keep_md: true
    highlight: tango
    # highlight_bw: false
    # md_extensions: +tex_math_dollars
    # latex_engine: xelatex
  html_document:
    keep_md: true
bibliography: references.bib  
---

```{r setup, include = FALSE}
library(knitr)
## knitr::knit_hooks$set(
##   prompt = function(before, options, envir) {
##     eng <- options$engine
##     if(eng %in% c("sh", "bash")) {
##       pr <- "$ "
##     } else if(eng == "R") {
##       pr <- "R> "
##     } else {
##       pr <- "> "
##     }
##     options(prompt = pr)
## })
## options(prompt = "R> ",
##         continue = "+  ",
##         width = 70,
##         useFancyQuotes = FALSE)
knitr::opts_chunk$set(comment="",
                      prompt = TRUE,
                      R.options = list(prompt = "R> ",
                                       continue = "+  ",
                                       width = 70,
                                       useFancyQuotes = FALSE))
embed_file <- function(file, lang) {
  cat(paste0("```", lang), readLines(file), "```", sep = "\n")
}
```

# Introduction

Empirical likelhood (EL) method allows statisticians to construct partially specified models via moment conditions. Although an EL model does not assume any parametric familiy of the data, the estimator is in some sense as efficient as a fully parametric model [@qin-lawless1994].

The empirical likelihood (EL) approach can be traced back to @thomas-grunkemeier1975. Its current framework is mainly developed by @owen1988, @owen1990, and @owen1991, where empirical likelihood ratio statistics is introduced, and the EL method is extended to linear regression models under fixed or random design. @kolaczyk1994 further generalize the method to be used with generalized linear models. @qin-lawless1994 relate estimating equation and empirical likelihood and provide asymptotic properties of the estimator.

A Bayesian approach to EL considers the pseudo-posterior distribution $p_{\EL}(\tth|Y) \propto \EL(\tth)\pi(\tth)$ where $\pi(\tth)$ is the prior distribution of $\tth$, is usually straightforward to explore by Markov chain Monte Carlo (MCMC) algorithm. However, notice that since EL is not a true likelihood, neither is $p_{\EL}(\tth|Y)$ a true posterior. The consequences of this have been investigated by e.g. @lazar2003. @chaudhuri-et-al2017 considers using Hamiltonian Monte Carlo sampling for the Bayesian EL models.

EL approach generally requires a convex hall condition, which means that a solution may not exist if this condition is not satisfied. @chen-et-al2008 propose an adjustment to the constraints in the EL framework to ensure a solution always exist, and the theoretical properties are not affected.

An approach related to EL is the so-called exponentially tilting (ET) method [@efron1981]. @schennach2005, and @schennach2007 propose the exponentially tilted empirical likelihood (ELET) approach, which enjoys the properties of both ET and EL methods. @newey-smith2004 also gives the theoretical results relating Generalized Method of Moment (GMM) and Generalized Empirical Likelihood (GEL), their higher order properties, as well as their bias-corrected forms in the absence of length-bias.

For length-biased data with EL, @zhou2005 proposes an EM algorithm for censored an truncated data under mean type constraints without covariates. @zhou-li2008 combine the empirical likelihood with the Buckley-James estimator which works for regression models. @zhou-et-al2012 revisit the fixed and random design linear regression models but for right-censored data and show that the model works well even with heteroscedastic errors. @shen-et-al2016 develop a different EM algorithm under the EL framework for one- or two- sample doubly censored data.

The construction of confidence regions or intervals under the EL frameworks has been mainly discussed when there is no length-bias. In this case, an asymptotic $\chi^2$ distribution of log EL is valid. When right-censoring is present, the asymptotic distribution is no longer a standard $\chi^2$ distribution but subject to an unknown scaling factor. Some approaches have been proposed with modifications of the estimating equations under the EL framework. For example, @he-et-al2016 consider using a special influence functions in the estimating equations to retain a standard $\chi^2$ distribution. @li-wang2003 propose an adjusted EL for linear regression using synthetic data approach. They extend the EL method for inference on a linear combination of the coefficients and also incorporate auxiliary information on the covariates. @ning-et-al2013 consider length-biased right-censored data in a non-regression setting for the estimation of mean, quantile and survival function of the population as well as confidence intervals.

Although the EL method has been extended and generalized over the years, there has not been a flexible and efficient software available to the public. The existing softwares are either written in a high-level programming language for which inner optimization is not efficient, or are designed for specific regression problems (e.g. \pkg{emplik}, \pkg{gelS4}). There is also no software that provides EL method for right-censored data.

This paper describes a framework we designed for EL researchers to develop fast and efficient implementations of their own EL models and related methods. We provide a computationally efficient R package called \pkg{flexEL} which is flexible enough for users to solve any type of regression problems with minimum programming effort. The computational efficiency is achieved by a C++ implementation of the Newtonâ€“Raphson algorithm which is a key step in the EL inner optimization problem. Other than the main functionality of reglular EL estimation, the package also provides support correction, continuity correction under right-censoring, gradient calculation, as well as various mean and quantile regression models for which the details will be described in the later sections.
	
# Methodology

## Basics

Let $\yy_1,\cdots,\yy_n$ where $\yy_i\in\R^{d+1}$ be iid observations from an unknown distribution $F_0(\yy)$, about which a parameter of interest $\tth$ is defined as satisfying an $m$-dimensional moment condition:
\begin{equation} \label{eq:momcond}
  \E\bigl[\gg(\yy;\tth)\bigr] = 0,
\end{equation}
where $\gg(\yy, \tth) = \bigl( g_1(\yy, \tth), \ldots, g_m(\yy, \tth) \bigr)$.

The empirical likelihood $\EL(\tth)$ is defined as the profile likelihood over the distribution function of $\yy$:
\begin{equation} \label{eq:elF}
  \EL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(\yy_i),
\end{equation}
where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:momcond}.

It was shown [@owen1988] that for any $\tth$, the maximum of \eqref{eq:elF} must be achieved by a PMF putting all mass on the support of the observed data $\yy_1, \cdots, \yy_n$, such that the infinite-dimensional profile likelihood \eqref{eq:elF} reduces to a finite-dimensional one:
\begin{equation}
  \EL(\tth) = \prod_{i=1}^n \hat{\w_i}(\tth),
\end{equation}
where the $n$-dimensional vector of probability weights $\hat{\w}(\tth)$ associated with the observations is the solution of an inner optimization problem which will be referred to as \textbf{EL inner optimization}
\begin{equation} \label{eq:noncensopt}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot \gg(\yy_i;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,n,
\end{split}
\end{equation}

The problem in (\ref{eq:noncensopt}) is a constrained convex optimization problem, and its optimal solution can be found via Lagrangian function, similar to the steps in @owen1990. Specifically, the Lagrangian funciton can be set up as 
\begin{equation} \label{eq:lagrange}
  \L = \sum_{i=1}^n \log(\w_i) + n\lla'(\sum_{i=1}^n \w_i \cdot g(\yy_i;\tth)) - \mu(\sum_{i=1}^n \w_i -1)
\end{equation}

Provided that $\bm 0$ is in the convex full of the points $\gg(\yy_1;\tth),\cdots,\gg(\yy_n;\tth)$, a unique optimal probability vector exist and can be shown to be
\begin{equation}
  \hat{\w_i}(\tth) = \frac{1}{n\cdot [1 - \hat{\lla}' (\tth) \gg(\yy_i;\tth)]},
\end{equation}
where the vector $\hat{\lla}(\tth)$ solves the unconstrained optimization problem
\begin{equation}
\hat{\l}(\tth) = \argmax_{\lla} \sum_{i=1}^n \str\log\Big(1 - \lla' \gg(\yy_i;\tth)\Big),
\end{equation}
and where
\begin{equation}
\str\log(x) = \begin{cases} \log(x) & x \ge \tfrac 1 n \\ -\tfrac 1 2 n^2 x^2 + 2 n x - \tfrac 3 2 - \log(n) & x < \tfrac 1 n \end{cases}.
\end{equation}

@qin-lawless1994 has shown that $\lla(\tth)$ is a continuous differentiable function of $\tth$ provided that convex hull condition is satisfied with $\tth$ and $\sum_{i=1}^n \gg(\yy_i;\tth)g'(\yy_i;\tth)$ is positive definite. However, the support of $\tth$ is not necessarily a convex set, as demonstrated by @chaudhuri-et-al2017.

In the case that we are only interested in one parameter $\theta$ of an unknown distribution $F$, @owen1990 has shown that the limiting distribution of the EL ratio statistic is chi-square. For a vector of parameters, @qin-lawless1994 shows that the EL ratio statistic is asymptotic normal. This means that we can derive confidence intervals of the paramters accordingly.

With the Lagrangian function in (\ref{eq:lagrange}) , we can also derive the gradient of $\L$ with respect to the matrix $G = [\gg(\yy_1;\tth), \cdots, \gg(\yy_n;\tth)]$, such that given the gradient of $G$ with respect to the paramters of interest $\tth$, we can obtain the gradient of L with respect to $\tth$, and with which we can employ a gradient based optimization algorithm to find the optimal solution of the estimator.

## Support Correction

As mentioned above, a necessary condition of obtaining a unique optimal solution is that $\bm 0$ is in the convex full of the points $\gg(\yy_1;\tth),\cdots,\gg(\yy_n;\tth)$ which may not be satisfied with the data. @chen-et-al2008 proposes a method to handle this situation by adding one more constraint in the EL inner optimization problem (\ref{eq:noncensopt}), that is

\begin{equation} \label{eq:noncensoptadj}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^{n+1} \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^{n+1} \w_i\cdot \gg_i(\tth) = 0 \\
  & \sum_{i=1}^{n+1} \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,{n+1},
\end{split}
\end{equation}

where $\gg_i(\tth) = \gg_i(y_i;\tth)$ for $i = 1,\cdots,n$, and $g_{n+1}(\tth) = -\frac{a_n}{n} \sum_{i=1}^n \gg_i(y_i;\tth)$ for some small positive $a_n$.

It is also shown by @chen-et-al2008 that this approach retains the optimally properties of EL, is faster to compute, and improves coverage probabilities of the confidence regions.

## Censoring

\pkg{flexEL} considers the situation of right-censoring, that is, instead of observing $y_i$, we observe $u_i = \min(y_i, c_i)$ and $\delta_i = \mathfrak 1\{y_i \le c_i\}$, where $c_i$ is the censoring time. In a regression setting where we also observe a set of covariates $\cx_i$ for each of the observations, we assume that the censoring variable $c_i$ is conditionally independent of $y_i$ given $\cx_i$. 

The right-censored empirical likelihood (CEL) is then defined as as the profile likelihood over the distribution function of $\uu$:
\begin{equation} \label{eq:celF}
  \CEL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(\uu_i)^{\delta_i} [1-\mathrm{d}F(\uu_i)]^{1-\delta_i},
\end{equation}
where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:momcond}.

It can be shown that with censored observations, it is no longer true that an optimal $F$ is only on the support of the data points $u_i, i=1,\cdots,n$. However, if we restrict ourselves to this case, we arrive at a finite dimensional problem
\begin{equation}
  \CEL(\tth) = \prod_{i=1}^n \Big[\hat \w_i(\tth)^{\delta_i}(\sum_{j: e_j \geq e_i}\hat \w_i(\tth))^{1-\delta_i}\Big].
\end{equation}
where similar as before, $\hat\w(\tth)$ is the solution of an inner optimization problem
\begin{equation} \label{eq:elcens}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n \Big[\delta_i\log(\w_i) + (1-\delta_i)\log(\sum_{j: e_j\geq e_i}\w_i)\Big]\\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot \gg(u_i;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1\cdots,n.
\end{split}
\end{equation}

Right-censoring is essentially a missing data problem. In \pkg{flexEL}, the right-censored EL problem is solved via an EM algorithm. The algorithm is a generalization of @zhou2005 to regression problems.

Recall that if $\delta_i=1$, we have $u_i=y_i$, otherwise we do not observe $y_i$. This means that the \textbf{unobserved (latent) variables} are the $y_i'$s such that $\delta_i=0$, the \textbf{complete data likelihood} is
\begin{equation}\label{eq:complike}
  \ell(\w,\yy|\uu) = \sum_{i=1}^n\log(\prod_{j=1}^n \w_j^{\mathfrak 1(y_i=u_j)})
\end{equation}

The E-step of the EM algorithm takes the expectation of (\ref{eq:complike}) with respect to $\yy$ (vector of all latent variables) conditioned on the observed values, the censoring indicator and the current state of the parameters, and since for $\delta_i = 1$, we know that $u_i=y_i$, we have
\begin{equation}\label{eq:estep}
\begin{split}
  \E_{\yy|\uu,\delta,\w_0}\bigl[\ell(\w,\yy|\uu)\bigr]
  &= \E_{\yy|\uu,\delta,\w_0}\Bigl[\sum_{i=1}^n\delta_i\log(\w_i) +
  (1-\delta_i)\log(\prod_{j=1}^n \w_j^{\mathfrak 1(y_i=u_j)})\Bigr] \\
  &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) +
  (1-\delta_i)\sum_{j=1}^n \E_{y_i|\uu,\delta,\w_0}[\mathfrak1(y_i=u_j)]\log(\w_j)\Bigr] \\
  &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) +
  (1-\delta_i)\sum_{j=1}^n P_{y_i|\uu,\delta,\w_0}(y_i=u_j)\log(\w_j)\Bigr].
\end{split}
\end{equation}

Notice that we can write
\[
  \log(\prod_{j=1}^n \w_j^{\mathfrak 1(y_i=u_j)}) =
  \sum_{i=1}^n \mathfrak 1(y_i=u_j)\log(\w_j),
\]
because for any $i \in \{1,\cdots,n\}$, $\mathfrak 1(y_i=u_j)=1$ for one and only one $j\in\{1,\cdots,n\}$. Also, the latent $y_i's$ are independent but not identically distributed, since each of them follows a different categorical distribution.

The conditional distribution in (\ref{eq:condprob_orig}) is a categorical distribution conditioned on that the probability mass only allocates on the values in a subset of $\{u_1,\cdots,u_n\}$ such that $\mathfrak 1(u_j\geq u_i) = 1$ for $j=1,\cdots,n$, which is still a multinomial distribution.
\begin{equation}\label{eq:condprob_orig}
  P_{y_i|\uu,\delta,\w_0}(y_i=u_j) =
  \frac{\mathfrak 1(u_j\geq u_i)\cdot \w_{0j}}{\sum_{k=1}^n\mathfrak 1(u_k\geq u_i)\cdot\w_{0k}}.
\end{equation}

Therefore, the EM algorithm iterates between the following two steps: (TODO)

# Illustrations

## Mean Regression

- Show users how to create their own `Gfun` and use this from \proglang{R}.  

- Can include gradient-based optimization here as well.  I suggest to use `optim(method = "BFGS")` for optimization, not `nlm()` (it's just more complicated/annoying).  I would say use `nlminb()` which is way faster than `optim()`, but for some reason R Core suggests to use the other optimizers instead...

## Quantile Regression

- What it is, smoothness correction, present built-in function for this.

## Built-in Models

- We have a bunch of these: quantile vs mean regression, and location vs location-scale.

- Censoring as well.

- Should we put all this in the Methods section?  Have separate illustrations for some or all?

## Bayesian EL

- Good to illustrate with \proglang{Stan} NUTS algorithm, because we've implemented it already and \proglang{Stan} is a really good autodiff engine + best NUTS implementation.

- Can illustrate with the example from the JRSSB EL-HMC paper.

# Conclusion

- Everything below here are just tests for using JSS Markdown.

# Numbering

## Equations

Here's a reference to equation \@ref(eq:xyz).
\begin{equation}
x + y = z.
(\#eq:xyz)
\end{equation}
Here's an unnumbered equation:
\[
a/b = c.
\]
Here's some inline math: $y = \exp(x^2 - 1)$.

## Figures

Reference to Figure \@ref(fig:plot).  Also an example of embedding \proglang{R} code.
```{r plot, fig.cap = "A simple plot."}
x <- {function(y) {
  y + 1:10
}}(3)
x
plot(x, pch = x, col = x)
```

## Tables

Reference to Table \@ref(tab:mtcars).
```{r mtcars, echo = FALSE}
knitr::kable(head(mtcars[, 1:8], 5), booktabs = TRUE,
             caption = "A table of the first 5 rows of the mtcars data.")
```

# Embedded Files

<!-- ## C++ File -->

\subsection[C++ File alpha + beta]{C++ File $\alpha + \beta$}

- LaTeX in heading doesn't seem to work.  [Here's](https://github.com/jgm/pandoc/issues/3555) how far I got with this.
- In fact, seems to be a pure LaTeX problem, in that `latexmk` compile of tex output takes several tries to get it right.
- Working solution: Use `\subsection[plaintext]{latex}` instead of Markdown section.
- New problem: `\tightlist` not defined...

```{r, echo = FALSE, results = "asis"}
embed_file("foo.h", "cpp")
```

## R Markdown File

```{r, echo = FALSE, results = "asis"}
embed_file("foo.Rmd", "bash")
```
