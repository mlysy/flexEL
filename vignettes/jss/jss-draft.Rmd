---
title:
  formatted: "\\pkg{flexEL}: A Fast and Flexible Framework for Empirical Likelihood Modeling"
  plain: "flexEL: A Fast and Flexible Framework for Empirical Likelihood Modeling"
author: 
  - name: Shimeng Huang
    affiliation: University of Waterloo
  - name: Martin Lysy
    affiliation: University of Waterloo
    address:
      - 200 University Avenue West
      - Ontario, Canada
    email: \email{mlysy@uwaterloo.ca}
abstract: >
  Here is the abstract.
keywords:
  # at least one keyword must be supplied
  formatted: [keywords, not capitalized, "\\proglang{Java}"]
  plain:     [keywords, not capitalized, Java]
date: "`r Sys.Date()`"
documentclass: jss
classoption: article
header-includes:
  - \usepackage{caption}
  - \captionsetup[table]{skip=.1em}
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \newcommand{\s}{\sigma}
  - \newcommand{\XX}{\bm X}
  - \newcommand{\xx}{\bm x}
  - \newcommand{\yy}{\bm y}
  - \newcommand{\zz}{\bm z}
  - \newcommand{\uu}{\bm u}
  - \newcommand{\qq}{\bm q}
  - \newcommand{\ee}{\bm e}
  - \newcommand{\tth}{\bm \theta}
  - \newcommand{\lla}{\bm \lambda}
  - \newcommand{\bbe}{\bm \beta}
  - \newcommand{\gga}{\bm \gamma}
  - \newcommand{\eep}{\bm \epsilon}
  - \newcommand{\oom}{\bm \omega}
  - \newcommand{\R}{\mathbb R}
  - \renewcommand{\gg}{\bm g}
  - \newcommand{\w}{\omega}
  - \renewcommand{\l}{\lambda}
  - \newcommand{\g}{\gamma}
  - \renewcommand{\L}{\mathcal{L}}
  - \renewcommand{\E}{\textrm{E}}
  - \newcommand{\EL}{\textrm{EL}}
  - \newcommand{\CEL}{\textrm{CEL}}
  - \newcommand{\str}[1]{{#1}^{\star}}
  - \DeclareMathOperator*{\argmax}{arg\,max}
  - \DeclareMathOperator*{\argmin}{arg\,min}
  - \newcommand{\cx}{\bm{{\scriptstyle \mathcal X}}}
  - \newcommand{\e}{\varepsilon}
  - \newcommand{\iid}{\stackrel {\textrm{iid}}{\sim}}
  - \renewcommand{\|}{\,|\,}
  - \newcommand{\sha}[1]{{#1}^{\sharp}}
output: 
  bookdown::pdf_book:
    toc: false
    template: jss-template.tex
    # keep_tex: true
    # keep_md: true
    highlight: tango
    # highlight_bw: false
    # md_extensions: +tex_math_dollars
    # latex_engine: xelatex
  html_document:
    keep_md: true
bibliography: references.bib  
---

```{r setup, include = FALSE}
library(knitr)
## knitr::knit_hooks$set(
##   prompt = function(before, options, envir) {
##     eng <- options$engine
##     if(eng %in% c("sh", "bash")) {
##       pr <- "$ "
##     } else if(eng == "R") {
##       pr <- "R> "
##     } else {
##       pr <- "> "
##     }
##     options(prompt = pr)
## })
## options(prompt = "R> ",
##         continue = "+  ",
##         width = 70,
##         useFancyQuotes = FALSE)
knitr::opts_chunk$set(comment="",
                      prompt = TRUE,
                      R.options = list(prompt = "R> ",
                                       continue = "+  ",
                                       width = 70,
                                       useFancyQuotes = FALSE))
embed_file <- function(file, lang) {
  cat(paste0("```", lang), readLines(file), "```", sep = "\n")
}
```

# Introduction

Empirical likelihood (EL) method allows statisticians to construct partially specified models via moment conditions. Although an EL model does not assume any parametric familiy of the data, the estimator is in some sense as efficient as a fully parametric model [@qin-lawless1994].

The empirical likelihood (EL) approach can be traced back to @thomas-grunkemeier1975. Its current framework is mainly developed by @owen1988, @owen1990, and @owen1991, where empirical likelihood ratio statistics is introduced, and the EL method is extended to linear regression models under fixed or random design. @kolaczyk1994 further generalize the method to be used with generalized linear models. @qin-lawless1994 relate estimating equation and empirical likelihood and provide asymptotic properties of the estimator. @chen-et-al2008 propose an adjustment to the constraints in the EL framework to ensure the convex hall condition is always satisfied and the theoretical properties are not affected. Moreover, @lazar2003 explores the validity of using empirical likelihood for Bayesian inference as well as the frequentist properties of the posterior intervals. @chaudhuri-et-al2017 considers using Hamiltonian Monte Carlo sampling for the Bayesian EL models.

<!-- A Bayesian approach to EL considers the pseudo-posterior distribution $p_{\EL}(\tth|Y) \propto \EL(\tth)\pi(\tth)$ where $\pi(\tth)$ is the prior distribution of $\tth$, is usually straightforward to explore by Markov chain Monte Carlo (MCMC) algorithm. However, notice that since EL is not a true likelihood, neither is $p_{\EL}(\tth|Y)$ a true posterior. The consequences of this have been investigated by e.g.  -->

An approach related to EL is the so-called exponentially tilting (ET) method [@efron1981]. @schennach2005, and @schennach2007 propose the exponentially tilted empirical likelihood (ELET) approach, which enjoys the properties of both ET and EL methods. @newey-smith2004 also gives the theoretical results relating Generalized Method of Moment (GMM) and Generalized Empirical Likelihood (GEL), their higher order properties, as well as their bias-corrected forms in the absence of length-bias.

For EL with length-biased data, @zhou2005 proposes an EM algorithm for censored and truncated data under mean type constraints without covariates. @zhou-li2008 combine the empirical likelihood with the Buckley-James estimator which works for regression models. @zhou-et-al2012 revisit the fixed and random design linear regression models but for right-censored data and show that the model works well even with heteroscedastic errors. @shen-et-al2016 develop a different EM algorithm under the EL framework for one- or two- sample doubly censored data.

Given fully observed data, an asymptotic $\chi^2$ distribution of log EL is valid. When right-censoring is present, the asymptotic distribution is no longer a standard $\chi^2$ distribution but subject to an unknown scaling factor. @he-et-al2016 consider using a special influence functions in the estimating equations to retain a standard $\chi^2$ distribution. @li-wang2003 propose an adjusted EL for linear regression using synthetic data approach.  @ning-et-al2013 consider length-biased right-censored data in a non-regression setting for the estimation of mean, quantile and survival function of the population as well as confidence intervals.

Although the EL method has been extended and generalized over the years, there has not been a flexible and efficient software available to the public. The existing softwares are either written in a high-level programming language for which inner optimization is not efficient, or are designed for specific regression problems (e.g. \pkg{emplik}, \pkg{gelS4}). There is also no software that provides EL method for right-censored data.

This paper describes a framework we designed for EL researchers to develop fast and efficient implementations of their own EL models and related methods. We provide a computationally efficient R package called \pkg{flexEL} which is flexible enough for users to solve any type of regression problems with minimum programming effort. The computational efficiency is achieved by a C++ implementation of the Newtonâ€“Raphson algorithm which is a key step in the EL inner optimization problem. Other than the main functionality of reglular EL estimation, the package also provides support correction, continuity correction under right-censoring, gradient calculation, as well as various mean and quantile regression models for which the details will be described in the later sections.
	
# Methodology

## Basics

Let $\XX = (\cx_1,\cdots,\cx_n)$ where $\cx_i\in\R^d$ be iid observations from an unknown distribution $F_0(\cx)$, about which a parameter of interest $\tth \in \R^p$ is defined as satisfying an $m$-dimensional moment condition:
\begin{equation} \label{eq:momcond}
  \E\bigl[\gg(\cx;\tth)\bigr] = 0,
\end{equation}
where $\gg(\cx, \tth) = \bigl( g_1(\cx, \tth), \ldots, g_m(\cx, \tth) \bigr)$.

The empirical likelihood $\EL(\tth)$ as the profile likelihood over the distribution function of $\cx$:
\begin{equation} \label{eq:elF}
  \EL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(\cx_i),
\end{equation}
where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:momcond}.

For any $\tth$, @owen1988 has shown that the maximum of \eqref{eq:elF} can be achieved by focusing on the distribution functions having all mass on the support of the observed data $\xx_1, \cdots, \xx_n$, and the infinite-dimensional profile likelihood \eqref{eq:elF} can be reduced to a finite-dimensional one. 

To have a general notation consistent with the EM algorithm under right-censoring in later sections, we now define \textbf{weighted empirical likelihood}:

\begin{equation}
  \EL(\tth) = \prod_{i=1}^n \hat{\w_i}(\tth)^{q_i},
\end{equation}

where $q_i > 0, \forall i=1,\cdots,n$, and the $n$-dimensional vector of probability weights $\hat{\w}(\tth)$ associated with the observations is the solution of an inner optimization problem which will be referred to as \textbf{EL inner optimization}:

\begin{equation} \label{eq:noncensopt}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n q_i \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot g_i(\XX;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,n.
\end{split}
\end{equation}

When $q_i = 1, \forall i=1,\cdots,n$, we have the regular empirical likelihood function. The problem in (\ref{eq:noncensopt}) is a constrained convex optimization problem, and its optimal solution can be found via Lagrangian function, similar to the steps in @owen1990. Specifically, the Lagrangian function can be set up as 
\begin{equation} \label{eq:lagrange}
  \L = \sum_{i=1}^n q_i\log(\w_i) + (\sum_{i=1}^n q_i)\lla'(\sum_{i=1}^n \w_i \cdot g_i(\XX;\tth)) + \mu(1-\sum_{i=1}^n \w_i).
\end{equation}

Let $r_i = q_i/\sum_{i=1}^n q_i$. Provided that $\bm 0$ is in the convex full of the points $g_1(\XX;\tth),\cdots,g_n(\XX;\tth)$, a unique optimal probability vector exist and can be shown to be

\begin{equation}\label{eq:omegas}
  \hat{\w_i}(\tth) = \frac{r_i}{1 - \hat{\lla}'(\tth) g_i(\XX;\tth)},
\end{equation}

where 

\begin{equation}
\begin{aligned}
\hat{\lla}(\tth) &= \argmax_{\lla(\tth)} \sum_{i=1}^n r_i\ \str\log\left(1 - \lla'(\tth) g_i(\XX;\tth); r_i\right), \\
\str\log(x; r) &= 
\begin{cases} 
\log(x) & x \ge r \\
- \frac{1}{2} (x/r)^2 + 2 (x/r) - \frac{3}{2} + \log r & x < r.
\end{cases}
\end{aligned}
\label{eq:optim}
\end{equation}

@qin-lawless1994 has shown that $\lla(\tth)$ is a continuous differentiable function of $\tth$ provided that convex hull condition is satisfied with $\tth$ and $\sum_{i=1}^n g_i(\XX;\tth)g_i'(\XX;\tth)$ is positive definite. However, the support of $\tth$ is not necessarily a convex set, as demonstrated by @chaudhuri-et-al2017.

In the case that we are only interested in one parameter $\theta$ of an unknown distribution $F$, @owen1990 has shown that the limiting distribution of the EL ratio statistic is chi-square. For a vector of parameters, @qin-lawless1994 shows that the EL ratio statistic is asymptotic normal. This means that we can derive confidence intervals of the paramters accordingly.

With the Lagrangian function in (\ref{eq:lagrange}), we can also derive the gradient of $\L$ with respect to $\tth$, with which we can employ a gradient based optimization algorithm to find the optimal solution of the estimator.

## Support Correction

As mentioned above, a necessary condition of obtaining a unique optimal solution is that $\bm 0$ is in the convex full of the points $g_i(\XX;\tth),\cdots,g_n(\XX;\tth)$ which may not be satisfied with the data. @chen-et-al2008 proposes a method to handle this situation by adding one more constraint in the EL inner optimization problem (\ref{eq:noncensopt}), that is

\begin{equation} \label{eq:noncensoptadj}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^{n+1} \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^{n+1} \w_i\cdot g_i(\XX;\tth) = 0 \\
  & \sum_{i=1}^{n+1} \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,{n+1},
\end{split}
\end{equation}

where $g_{n+1}(\XX;\tth) = -\frac{a_n}{n} \sum_{i=1}^n g_i(\XX;\tth)$ for some small positive $a_n$.

It is also shown by @chen-et-al2008 that this approach retains the optimally properties of EL, is faster to compute, and improves coverage probabilities of the confidence regions.

## Regression with Right-Censored Outcome

### Empirical Likelihood with Right-Censored Outcome Variable

\pkg{flexEL} handles the situation of right-censored regression, that is, instead of observing outcomes $y_i$, we observe $u_i = \min(y_i, c_i)$ and $\delta_i = \mathfrak 1\{y_i \le c_i\}$, where $c_i$ is the censoring time. 

Consider a general regression model
\[
  y_i = f(\cx_i; \tth) + \e_i, \quad i=1,\cdots,n
\]
where $\e_i \iid (0,1)$ and independent of $\cx_i$, and $m$-dimensional conditional moment restrictions
\begin{equation}\label{eq:cmom}
  \E\bigl[\gg(\cx, \e; \tth) \| \cx\bigr] = 0.
\end{equation}

We assume that the censoring variable $c_i$ is conditionally independent of $y_i$ given $\cx_i$. Let's denote the residuals given a specific $\tth$ as $e_i'$s (corresponding to $u_i'$s), the complete residuals as $\e_i'$s (corresponding to $y_i'$s). 

The empirical likelihood with censored observations once again is defined by profiling over the unknown joint distribution function $F(\cx,\e) = G(\cx) \cdot H(\e)$, where $G$ and $H$ are the CDFs of $\cx$ and $\e$:
\begin{equation}\label{eq:celF}
  \CEL(\tth) = \max_{F \in \mathcal F(\tth)}\prod_{i=1}^n dG(\cx_i) \cdot dH(e_i)^{\delta_i} \cdot [1- H(e_i)]^{1-\delta_i},
\end{equation}
where
\[
  e_i = e_i(\tth) = \frac{u_i - \mu(\cx_i;\tth)}{\eta(\cx_i;\tth)},
\]
and $\mathcal F(\tth)$ is the set of all valid distribution functions satisfying \eqref{eq:cmom}.  It is not hard to show that for any choice of $H(\e)$, the maximum of \eqref{eq:celF} over $G(\cx)$ is attained as the empirical distribution $\hat G(\cx)$ which puts a point mass of $1/n$ on each covariate observation $\cx_1, \ldots, \cx_n$.  Restricting our attention to $G(\cx)$ uniform on the observed covariates, and considering only the weaker moment condition
\begin{equation}\label{eq:umom}
  \E\bigl[\gg(\cx, \e; \tth)\bigr] = 0
\end{equation}
(which is true for any $G(\cx)$ if \eqref{eq:cmom} holds), the CEL function reduces to
\[
  \CEL(\tth) = \max_{F \in \mathcal F^\star(\tth)}\prod_{i=1}^n dH(e_i)^{\delta_i} \cdot [1- H(e_i)]^{1-\delta_i},
\]
where $\mathcal F^\star(\tth)$ is the set of all valid distributions $F(\cx, \e)$ satisfying \eqref{eq:umom}.

<!-- The right-censored empirical likelihood (CEL) is then defined as as the profile likelihood over the distribution function of $\ee$: -->
<!-- \begin{equation} \label{eq:celF} -->
<!--   \CEL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(e_i)^{\delta_i} [1-\mathrm{d}F(e_i)]^{1-\delta_i}, -->
<!-- \end{equation} -->
<!-- where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:cmom}. -->

It can be shown that with censored observations, it is no longer true that an optimal $F$ has support only on the data points $e_i, i=1,\cdots,n$. However, if we restrict ourselves to this case, we arrive at a finite dimensional problem
\begin{equation}
  \CEL(\tth) = \prod_{i=1}^n \Big[\hat \w_i(\tth)^{\delta_i}(\sum_{j: e_j \geq e_i}\hat \w_i(\tth))^{1-\delta_i}\Big].
\end{equation}

where similar as before, $\hat\w(\tth)$ is the solution of an inner optimization problem

\begin{equation} \label{eq:elcens}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n \Big[\delta_i\log(\w_i) + (1-\delta_i)\log(\sum_{j: e_j\geq e_i}\w_i)\Big]\\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot g_i(\yy,\cx;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1\cdots,n.
\end{split}
\end{equation}

Right-censoring is essentially a missing data problem. In \pkg{flexEL}, the right-censored EL problem is solved via an EM algorithm. The algorithm is a generalization of @zhou2005 to regression problems.

### An EM Algorithm

Recall that if $\delta_i=1$, we have $e_i=\e_i$. This means that the \textbf{unobserved (latent) variables} are the $\e_i'$s such that $\delta_i = 0$, the \textbf{complete data likelihood} is
\begin{equation}\label{eq:complike}
  \ell(\w,\e|\uu) = \sum_{i=1}^n\log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)})
\end{equation}

The E-step of the EM algorithm takes the expectation of (\ref{eq:complike}) with respect to $\yy$ (vector of all latent variables) conditioned on the observed values, the censoring indicator and the current state of the parameters, and since for $\delta_i = 1$, we know that $u_i=y_i$, we have

\begin{equation}\label{eq:estep}
\begin{split}
  \E_{\e|e,\delta,\w_0}\bigl[\ell(\w,\e|e)\bigr]
  &= \E_{\e|e,\delta,\w_0}\Bigl[\sum_{i=1}^n\delta_i\log(\w_i) +
  (1-\delta_i)\log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)})\Bigr] \\
  &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) +
  (1-\delta_i)\sum_{j=1}^n \E_{\e_i|e,\delta,\w_0}[\mathfrak1(\e_i=e_j)]\log(\w_j)\Bigr] \\
  &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) +
  (1-\delta_i)\sum_{j=1}^n P_{\e_i|e,\delta,\w_0}(\e_i=e_j)\log(\w_j)\Bigr].
\end{split}
\end{equation}

Notice that we can write
\[
  \log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)}) =
  \sum_{i=1}^n \mathfrak 1(\e_i=e_j)\log(\w_j),
\]
because for any $i \in \{1,\cdots,n\}$, $\mathfrak 1(\e_i=e_j)=1$ for one and only one $j\in\{1,\cdots,n\}$. Also, the latent $\e_i's$ are independent but not identically distributed, since each of them follows a different categorical distribution (multinomial distribution with one trial).

The conditional distribution in (\ref{eq:condprob_orig}) is a categorical distribution conditioned on that the probability mass only allocates on the values in a subset of $\{e_1,\cdots,e_n\}$ such that $\mathfrak 1(e_j\geq e_i) = 1$ for $j=1,\cdots,n$, which is still a multinomial distribution.
\begin{equation}\label{eq:condprob_orig}
  P_{\e_i|e,\delta,\w_0}(\e_i=e_j) =
  \frac{\mathfrak 1(e_j\geq e_i)\cdot \w_{0j}}{\sum_{k=1}^n\mathfrak 1(e_k\geq e_i)\cdot\w_{0k}}.
\end{equation}

Therefore, the EM algorithm iterates between the following two steps:

\begin{itemize}
\item \textbf{E-step:} Given the observed values and the weights $\w_0$ from the previous iteration, the expectation of the log likelihood is
\begin{equation}\label{eq:emestep}
\begin{split}
  \E_{\e|e,\delta,\w_0}[\ell(\w,\e|e)]
  &= \sum_{i=1}^n \Big[\delta_i \log \w_i +
    (1-\delta_i) \sum_{j: e_j\geq e_i} \tilde \w_{ij} \log \w_j\Big] \\
  &= \sum_{i=1}^n \Big[\delta_i + \sum_{k: e_k\leq e_i} (1-\delta_k)
      \cdot\tilde\w_{ki}\Big]\cdot\log \w_i,
\end{split}
\end{equation}
where
\[
  \tilde \w_{ki} = \frac{\w_{0i}}{\sum_{l: e_l\geq e_k}\w_{0l}}, \quad k,i: e_k\leq e_i.
\]

\item \textbf{M-step:}
Let $q_i = \delta_i + \sum_{k: e_k\leq e_i} (1-\delta_k)\cdot \tilde \w_{ki}$ for $i=1,\cdots,n$, then the problem becomes
\begin{equation}\label{eq:emmstep}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n q_i \log \w_i \\
  \mbox{s.t.}\quad & \sum_{i=1}^n \w_i\cdot \gg(\cx_i,u_i;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1\cdots,n,
\end{split}
\end{equation}

which has the form of the weighted empirical likelihood inner optimization problem, and $\oom = (\w_1,\cdots,\w_n)$ can be updated as given by equations (\ref{eq:omegas}) to (\ref{eq:optim}).

\end{itemize}

# Illustrations

## Mean Regression

- Show users how to create their own `Gfun` and use this from \proglang{R}.  

- Can include gradient-based optimization here as well.  I suggest to use `optim(method = "BFGS")` for optimization, not `nlm()` (it's just more complicated/annoying).  I would say use `nlminb()` which is way faster than `optim()`, but for some reason R Core suggests to use the other optimizers instead...

## Quantile Regression

- What it is, smoothness correction, present built-in function for this.

For the location-scale model \eqref{md:lsmod}, the $\tau\times 100\%$ conditional quantile of $y_i$ is
\begin{equation} \label{eq:lsqr}
  Q_{\tau}(y_i|\cx_i) = \xx_i' \bbe + \s\cdot\exp(\zz_i' \gga)\cdot\nu_{\tau}.
\end{equation}

In this case, for parameters $\bbe,\gga$ and $\s$, we adopt the same estimating equation as in the mean regression case. For the quantile parameter $\nu_{\tau}$, we rely on the ``check function" introduced by \cite{koenker-bassett1978}, which is defined as
\begin{equation}\label{eq:check}
\rho_\tau(u) = u\cdot(\tau - \mathfrak 1 \{u \le 0\}),
\end{equation}
where $\mathfrak 1\{\cdot\}$ is the indicator function.

If the $\tau$-th quantile value of $\e_i\iid(0,1)$ is $\nu_{\tau}$, then $\e_i-\nu_{\tau}$ has $\tau$-th quantile value $0$. The estimator of $\nu_{\tau}$ is then defined as
\begin{equation}\label{eq:nueq}
  \hat\nu_{\tau} = \argmin_{\tilde\nu_{\tau}} \E\Biggl[\rho_{\tau}\Bigl(\frac{y-\xx'\bbe}{\s\cdot\exp(\zz'\g)}-\tilde\nu_{\tau}\Bigr)\Biggr].
\end{equation}

As before, we use the first order optimality condition of (\ref{eq:nueq}) to obtain the estimating equation for $\nu_{\tau}$. Therefore, we obtain all the moment conditions for quantile regression as follows
\begin{equation} \label{eq:qrls}
\begin{split}
  \E\Bigl[\frac{y-\xx'\bbe}{\exp(2\zz'\gga)}\cdot\xx\Bigr] &= 0 \\
  \E\Bigl[\bigl(1-\frac{(y-\xx'\bbe)^2}{\s^2\cdot\exp(2\zz'\gga)}\bigr)\cdot\zz\Bigr] &= 0 \\
  \E\Bigl[\frac{(y-\xx'\bbe)^2}{\s^2\cdot\exp(2\zz'\gga)}-1\Bigr] &= 0 \\
  \E\Bigl[\rho'_{\tau}\bigl(\frac{y-\xx'\bbe}{\s\cdot\exp(\zz'\gga)}-\nu_{\tau}\bigr)\Bigr] &= 0.
\end{split}
\end{equation}

## Built-in Models

- We have a bunch of these: quantile vs mean regression, and location vs location-scale.

- Censoring as well.

- Should we put all this in the Methods section?  Have separate illustrations for some or all?

## Bayesian EL

- Good to illustrate with \proglang{Stan} NUTS algorithm, because we've implemented it already and \proglang{Stan} is a really good autodiff engine + best NUTS implementation.

- Can illustrate with the example from the JRSSB EL-HMC paper.

# Conclusion

- Everything below here are just tests for using JSS Markdown.

# Numbering

## Equations

Here's a reference to equation \@ref(eq:xyz).
\begin{equation}
x + y = z.
(\#eq:xyz)
\end{equation}
Here's an unnumbered equation:
\[
a/b = c.
\]
Here's some inline math: $y = \exp(x^2 - 1)$.

## Figures

Reference to Figure \@ref(fig:plot).  Also an example of embedding \proglang{R} code.
```{r plot, fig.cap = "A simple plot."}
x <- {function(y) {
  y + 1:10
}}(3)
x
plot(x, pch = x, col = x)
```

## Tables

Reference to Table \@ref(tab:mtcars).
```{r mtcars, echo = FALSE}
knitr::kable(head(mtcars[, 1:8], 5), booktabs = TRUE,
             caption = "A table of the first 5 rows of the mtcars data.")
```

# Embedded Files

<!-- ## C++ File -->

\subsection[C++ File alpha + beta]{C++ File $\alpha + \beta$}

- LaTeX in heading doesn't seem to work.  [Here's](https://github.com/jgm/pandoc/issues/3555) how far I got with this.
- In fact, seems to be a pure LaTeX problem, in that `latexmk` compile of tex output takes several tries to get it right.
- Working solution: Use `\subsection[plaintext]{latex}` instead of Markdown section.
- New problem: `\tightlist` not defined...

```{r, echo = FALSE, results = "asis"}
embed_file("foo.h", "cpp")
```

## R Markdown File

```{r, echo = FALSE, results = "asis"}
embed_file("foo.Rmd", "bash")
```
