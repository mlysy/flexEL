---
title:
  formatted: "\\pkg{flexEL}: A Fast and Flexible Framework for Empirical Likelihood Modeling"
  plain: "flexEL: A Fast and Flexible Framework for Empirical Likelihood Modeling"
author: 
  - name: Shimeng Huang
    affiliation: University of Waterloo
  - name: Yunfeng Yang
    affiliation: University of Waterloo
  - name: Silvia Meng
    affiliation: University of Waterloo
  - name: Martin Lysy
    affiliation: University of Waterloo
    address:
      - Department of Statistics and Actuarial Science
      - University of Waterloo
      - 200 University Avenue West
      - Waterloo, Canada
    email: \email{mlysy@uwaterloo.ca}
abstract: >
  This paper introduces \pkg{flexEL}, a fast and flexible framework for implementing and calibrating empirical likelihood models. In particular, it provides the loglikelihood and gradient functions for arbitrary moment constraint matrices. The inner optimization problem is efficiently computed in C++ using the "Eigen" linear algebra library. The package also provides functions for implementing right-censored regression models, where the inner optimization is conducted via an expectation-maximation algirithm. Users may interface with the library through \proglang{R} or directly through C++, as the underlying C++ code is exposes as a standalone header-only library.
keywords:
  # at least one keyword must be supplied
  formatted: [empirical likelihood, quantile regression, EM algorithm, right-censoring, location-scale model, "\\proglang{R}"]
  plain:     [empirical likelihood, quantile regression, EM algorithm, right-censoring, location-scale model, R]
date: "`r Sys.Date()`"
documentclass: jss
classoption: article
output: 
  bookdown::pdf_document2:
    toc: false
    template: jss-template.tex
    includes:
      in_header: jss-includes.tex
    citation_package: natbib
    # keep_tex: true
    highlight: tango
---
\newcommand{\logit}{\operatorname{logit}}
\newcommand{\ilogit}{\operatorname{ilogit}}
\newcommand{\rrh}{\boldsymbol{\rho}}
\newcommand{\mmu}{\boldsymbol{\mu}}
\newcommand{\nnu}{\boldsymbol{\nu}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\logel}{\operatorname{logEL}}



```{r setup, include = FALSE}
library(knitr)
knitr::opts_chunk$set(comment="",
                      prompt = TRUE,
                      R.options = list(prompt = "R> ",
                                       continue = "+  ",
                                       width = 70,
                                       useFancyQuotes = FALSE))

#' Embed arbitary code files and typset them correctly. 
embed_file <- function(file, lang) {
  cat(paste0("```", lang), readLines(file), "```", sep = "\n")
}
```

# Introduction

Empirical likelihood (EL) method allows statisticians to construct partially specified models via moment conditions. Although an EL model does not assume any parametric family of the data, the estimator are in some sense as efficient as a fully parametric model [@qin-lawless1994].

The empirical likelihood (EL) approach can be traced back to @thomas-grunkemeier1975. Its current framework is mainly developed by @owen1988, @owen1990, and @owen1991, where empirical likelihood ratio statistics is introduced, and the EL method is extended to linear regression models under a fixed or random design. @kolaczyk1994 further generalize the method so that it be used with generalized linear models. @qin-lawless1994 relate estimating equation and empirical likelihood and provide asymptotic properties of the estimator. @chen-et-al2008 propose an adjustment to the constraints in the EL framework to ensure that the convex hall condition is always satisfied when the theoretical properties are not affected. Moreover, @lazar2003 explores the validity of using empirical likelihood for Bayesian inference as well as the frequentist properties of the posterior intervals. @chaudhuri-et-al2017 consider using Hamiltonian Monte Carlo sampling for the Bayesian EL models.

<!-- A Bayesian approach to EL considers the pseudo-posterior distribution $p_{\EL}(\tth|Y) \propto \EL(\tth)\pi(\tth)$ where $\pi(\tth)$ is the prior distribution of $\tth$, is usually straightforward to explore by Markov chain Monte Carlo (MCMC) algorithm. However, notice that since EL is not a true likelihood, neither is $p_{\EL}(\tth|Y)$ a true posterior. The consequences of this have been investigated by e.g.  -->

An approach related to EL is the so-called exponentially tilting (ET) method [@efron1981]. @schennach2005, and @schennach2007 propose the exponentially tilted empirical likelihood (ELET) approach, which enjoys the properties of both ET and EL methods. @newey-smith2004 also give the theoretical results relating Generalized Method of Moment (GMM) and Generalized Empirical Likelihood (GEL), their higher order properties, as well as their bias-corrected forms in the absence of length-bias.

For EL with length-biased data, @zhou2005 proposes an EM algorithm for censored and truncated data under mean type constraints without covariates. @zhou-li2008 combine the empirical likelihood with the Buckley-James estimator which works for regression models. @zhou-et-al2012 revisit the fixed and random design linear regression models but for right-censored data and show that the model works well even with heteroscedastic errors. @shen-et-al2016 develop a different EM algorithm under the EL framework for one- or two- sample doubly censored data.

Given fully observed data, an asymptotic $\chi^2$ distribution of log EL is valid. When right-censoring is present, the asymptotic distribution is no longer a standard $\chi^2$ distribution but subject to an unknown scaling factor. @he-et-al2016 consider using a special influence function in the estimating equations to retain a standard $\chi^2$ distribution. @li-wang2003 propose an adjusted EL for linear regression using a synthetic data approach.  @ning-et-al2013 consider length-biased right-censored data in a non-regression setting for the estimation of mean, quantile and survival function of the population as well as confidence intervals.

Although the EL method has been extended and generalized over the years, there has not been a flexible and efficient software available to the public. The existing softwares are either written in a high-level programming language for which inner optimization is not efficient, or designed for specific regression problems (e.g. \pkg{emplik}, \pkg{gelS4}). There is also no software that provides EL method for right-censored data.

This paper describes a framework we designed for EL researchers to develop fast and efficient implementations of their own EL models and related methods. We provide a computationally efficient R package called \pkg{flexEL} which is flexible enough for users to solve any type of regression problems with minimum programming efforts. The computational efficiency is achieved by a C++ implementation of the Newtonâ€“Raphson algorithm which is a key step in the EL inner optimization problem. Including the solution to regular EL estimation, the package also provides support correction, continuity correction under right-censoring, gradient calculation, as well as various mean and quantile regression models for which the details will be described in the later sections.

# Methodology

## Basics

Let $\XX = (\xx_1,\cdots,\xx_n)$ be the data where $\xx_i\in\R^d$'s, $i = 1,...,n$, are independent and identically distributed observations from an unknown distribution $F_0(\xx)$. Then a parameter of interest $\tth \in \R^p$ is defined to satisfy an $m$-dimensional moment condition:
\begin{equation} \label{eq:momcond}
  \E\bigl[\gg(\xx;\tth)\bigr] = 0,
\end{equation}
where $\gg(\xx, \tth) = \bigl( g_1(\xx, \tth), \ldots, g_m(\xx, \tth) \bigr)$.

The empirical likelihood $\EL(\tth)$ is defined as the profile likelihood over the distribution function of $\xx$:
\begin{equation} \label{eq:elF}
  \EL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(\xx_i),
\end{equation}
where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:momcond}.

For any $\tth$, @owen1988 has shown that the maximum of \eqref{eq:elF} can be achieved by focusing on the distribution functions having all mass on the support of the observed data $\xx_1, \cdots, \xx_n$, and the infinite-dimensional profile likelihood \eqref{eq:elF} can be reduced to a finite-dimensional one. 

To have a general notation consistent with the EM algorithm under right-censoring in later sections, we now define \textbf{weighted empirical likelihood}:

\begin{equation}
  \EL(\tth) = \prod_{i=1}^n \hat{\w_i}(\tth)^{q_i},
\end{equation}

where $q_i > 0, \forall i=1,\cdots,n$, and the $n$-dimensional vector of probability weights $\hat{\w}(\tth)$ associated with the observations is the solution of an inner optimization problem which will be referred to as \textbf{EL inner optimization}:

\begin{equation} \label{eq:noncensopt}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n q_i \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot g(\xx_i;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,n.
\end{split}
\end{equation}

When $q_i = 1, \forall i=1,\cdots,n$, we have the regular empirical likelihood function. The problem in (\ref{eq:noncensopt}) is a constrained convex optimization problem, and its optimal solution can be found via Lagrangian function, similar to the steps in @owen1990. Specifically, the Lagrangian function can be set up as 
\begin{equation} \label{eq:lagrange}
  \L = \sum_{i=1}^n q_i\log(\w_i) + (\sum_{i=1}^n q_i)\lla'(\sum_{i=1}^n \w_i \cdot g(\xx_i;\tth)) + \mu(1-\sum_{i=1}^n \w_i).
\end{equation}

Provided that $\bm 0$ is in the convex full of the points $g(\xx_i;\tth),\cdots,g(\xx_n;\tth)$, a unique optimal probability vector exists and can be shown to be

\begin{equation}\label{eq:omegahat}
  \hat{\w_i}(\tth) = \frac{r_i}{1 - \hat{\lla}'(\tth) g(\xx_i;\tth)},
\end{equation}

where 

\begin{equation}
\begin{aligned}
r_i &= q_i/\sum_{i=1}^n q_i, \\
\hat{\lla}(\tth) &= \argmax_{\lla(\tth)} \sum_{i=1}^n r_i\ \str\log\left(1 - \lla'(\tth) g(\xx_i;\tth); r_i\right), \\
\str\log(x; r) &= 
\begin{cases} 
\log(x) & x \ge r \\
- \frac{1}{2} (x/r)^2 + 2 (x/r) - \frac{3}{2} + \log r & x < r.
\end{cases}
\end{aligned}
\label{eq:optim}
\end{equation}

@qin-lawless1994 have shown that $\lla(\tth)$ is a continuous differentiable function of $\tth$ provided that convex hull condition is satisfied with $\tth$ and $\sum_{i=1}^n g(\xx_i;\tth)g'(\xx_i;\tth)$ is positive definite. However, the support of $\tth$ is not necessarily a convex set, as demonstrated by @chaudhuri-elhmc.

In the case that we are only interested in one parameter $\theta$ of an unknown distribution $F$, @owen1990 has shown that the limiting distribution of the EL ratio statistic is chi-square. For a vector of parameters, @qin-lawless1994 show that the EL ratio statistic is asymptotic normal. This means that we can derive confidence intervals of the parameters accordingly.

With the Lagrangian function in (\ref{eq:lagrange}), we can also derive the gradient of $\L$ with respect to $\tth$, which allows one to employ a gradient-based optimization algorithm to find the optimal solution of the estimator. Specifically, the gradient of the the weighted empirical log likelihood with respect to $\tth$ can be derived as

\begin{equation}
\dth \log \EL(\tth) = Q \sum_{i=1}^n \hat \omega_i(\tth) \cdot \lla(\tth)' \dth g_i(\tth),
\end{equation}

where $Q = \sum_{i=1}^n q_i$.  

## Support Correction

Let $g_i = g(\xx_i;\tth)$ for $i = 1,\cdots, n$. As mentioned above, a necessary condition of obtaining a unique optimal solution is that $\bm 0$ is in the convex full of the points $g_1,\cdots,g_n$ which may not be satisfied with the data. @chen-et-al2008 propose a method to handle this situation by adding one more constraint in the EL inner optimization problem (\ref{eq:noncensopt}), that is

\begin{equation} \label{eq:noncensoptadj}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^{n+1} q_i \log(\w_i) \\
  \text{s.t.}\quad & \sum_{i=1}^{n+1} \w_i\cdot g_i = 0 \\
  & \sum_{i=1}^{n+1} \w_i = 1 \\
  & \w_i \geq 0, \quad i=1,\cdots,{n+1},
\end{split}
\end{equation}

where $g_{n+1} = -\frac{a_n}{n} \sum_{i=1}^n g_i$ for some small positive $a_n$.

It is also shown by @chen-et-al2008 that this approach retains the optimally properties of EL, is faster to compute, and improves coverage probabilities of the confidence regions.
<!-- The advantages of the approach, shown by @chen-et-al2008, include the retained optimal properties of EL, the faster computation, and the improved coverage probabilities of the confidence regions. -->


## Regression with Right-Censored Outcome

### Setup

The package \pkg{flexEL} handles the situation of regression models with right-censored outcomes, that is, instead of observing outcomes $y_i$, we observe $u_i = \min(y_i, c_i)$ and $\delta_i = \mathfrak 1\{y_i \le c_i\}$, where $c_i$ is the censoring time. The censoring variable $c_i$ is assumed to be conditionally independent of $y_i$ given $\xx_i$.

Consider a general regression model
\[
  y_i = \mu(\xx_i; \tth) + \eta(\xx_i; \tth)\cdot\e_i, \quad i=1,\cdots,n
\]
where $\e_i$ follows an unknown distribution with mean $0$ and variance $1$, and independent of $\xx_i$.

Let us denote the residuals given a specific $\tth$ as $e_i'$s (corresponding to $u_i'$s) and the complete residuals as $\e_i'$s (corresponding to $y_i'$s). The empirical likelihood with censored observations once again is defined by profiling over the unknown joint distribution function $F(\xx,\e) = G(\xx) \cdot H(\e)$, where $G$ and $H$ are the CDFs of $\xx$ and $\e$:
\begin{equation}\label{eq:celF}
  \CEL(\tth) = \max_{F \in \mathcal F(\tth)}\prod_{i=1}^n dG(\xx_i) \cdot dH(\e_i)^{\delta_i} \cdot [1- H(e_i)]^{1-\delta_i},
\end{equation}
<!-- where -->
<!-- \[ -->
<!--   e_i = e_i(\tth) = \frac{u_i - \mu(\xx_i;\tth)}{\eta(\xx_i;\tth)}, -->
<!-- \] -->
where $e_i = e_i(\tth) = \{u_i - \mu(\xx_i;\tth)\}/\eta(\xx_i;\tth)$,
and $\mathcal F(\tth)$ is the set of all valid distribution functions.  It is not hard to show that for any choice of $H(\e)$, the maximum of \eqref{eq:celF} over $G(\xx)$ is attained as the empirical distribution $\hat G(\xx)$ which puts a point mass of $1/n$ on each covariate observation $\xx_1, \ldots, \xx_n$.  Restricting our attention to $G(\xx)$ uniform on the observed covariates, and considering the moment condition
\begin{equation}\label{eq:umom}
  \E\bigl[\gg(\xx, \e; \tth)\bigr] = 0
\end{equation}
(which is true for any $G(\xx)$ if \eqref{eq:cmom} holds), the CEL function reduces to
\begin{equation}
  \CEL(\tth) = \max_{F \in \mathcal F^\star(\tth)}\prod_{i=1}^n dH(e_i)^{\delta_i} \cdot [1- H(e_i)]^{1-\delta_i},
\end{equation}
where $\mathcal F^\star(\tth)$ is the set of all valid distributions $F(\xx, \e)$ satisfying \eqref{eq:umom}.

<!-- The right-censored empirical likelihood (CEL) is then defined as as the profile likelihood over the distribution function of $\ee$: -->
<!-- \begin{equation} \label{eq:celF} -->
<!--   \CEL(\tth) = \max_{F \in \mathcal F(\tth)} \prod_{i=1}^n \mathrm{d} F(e_i)^{\delta_i} [1-\mathrm{d}F(e_i)]^{1-\delta_i}, -->
<!-- \end{equation} -->
<!-- where for any given $\tth$, $\mathcal F(\tth)$ is the set of (valid) distribution functions satisfying \eqref{eq:cmom}. -->

### Empirical Likelihood with Right-Censored Outcome Variable

It can be shown that with censored observations, it is no longer true that an optimal $F$ has support only on the data points $e_i, i=1,\cdots,n$. However, if we restrict ourselves to this case, we arrive at a finite dimensional problem
\begin{equation} \label{eq:elcens}
  \CEL(\tth) = \prod_{i=1}^n \Big[\hat \w_i(\tth)^{\delta_i}(\sum_{j: e_j \geq e_i}\hat \w_i(\tth))^{1-\delta_i}\Big].
\end{equation}

where similar as before, $\hat\w(\tth)$ is the solution of an inner optimization problem

\begin{equation} \label{eq:elcens_inner}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n \Big[\delta_i\log(\w_i) + (1-\delta_i)\log(\sum_{j: e_j\geq e_i}\w_i)\Big]\\
  \text{s.t.}\quad & \sum_{i=1}^n \w_i\cdot g_i(\yy,\xx;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1\cdots,n.
\end{split}
\end{equation}

Right-censoring is essentially a missing data problem. In \pkg{flexEL}, the right-censored EL problem is solved via an EM algorithm. The algorithm is a generalization of @zhou2005 to regression problems.



### Continuity Correction of Empirical Likelihood with Right-Censored Outcomes

It turns out that the log of equation (\ref{eq:elcens}) (log CEL) is not a continuous function in $\tth$, so that direct optimization of log CEL is difficult to achieve. Therefore, we develop a continuity correction method, which is a revision of the log CEL function, so that one can employ gradient-based optimization algorithms.


<!-- In order to obtain an estimate in this case, one could use Markov Chain Monte Carlo or global optimization algorithm such as simulated annealing, which are both time-consuming.  -->


The log CEL can be expanded as follows
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \ell_{\CEL}(\tth) &= \sum_{i=1}^n \Bigl[\delta_i\log(\w_i(\tth)) + -->
<!--   (1-\delta_i)\log(\sum_{j:e_j \geq e_i} \w_j(\tth))\Bigr] \\ -->
<!--   &= \sum_{i=1}^n \Bigl[\delta_i\log(\w_i(\tth)) + -->
<!--     (1-\delta_i)\log(\sum_{j=1}^n \mathfrak 1(e_j(\tth) \geq e_i(\tth)) \cdot \w_j(\tth))\Bigr]. -->
<!-- \end{split} -->
<!-- \label{eq:logel_orig} -->
<!-- \end{equation} -->
\begin{equation}
\ell_{\CEL}(\tth) 
  = \sum_{i=1}^n \Bigl[\delta_i\log(\w_i(\tth)) +
    (1-\delta_i)\log(\sum_{j=1}^n \mathfrak 1(e_j(\tth) \geq e_i(\tth)) \cdot \w_j(\tth))\Bigr].
\label{eq:logel_orig}
\end{equation}

We can see that the discontinuity of $\ell_{\CEL}(\tth)$ in (\ref{eq:logel_orig}) is due to an indicator function. To correct this discontinuity, we replace the indicator function by a continuous approximation.

Let $S$ be a transformed sigmoid function, i.e.,
\begin{equation}\label{eq:sigmoid}
  S(x; s) = \frac{1}{1+\exp(s\cdot x)},
\end{equation}
where $s > 0$ is a smoothing parameter. A plot of the function is given in Figure \ref{fig:smoothfun}. This function is radially symmetric around the point $(0,0.5)$. The smoothing parameter $s$ is default to 10 in **flexEL**.

```{r smoothfun, echo = FALSE, fig.height = 3.5, fig.cap = "Smoothed indicator function with different levels of smoothing parameter $s$."}
smooth_ind <- function(x, s) 1/(1 + exp(s * x))
par(mar = c(4.5, 4.5, .5, .5))
curve(smooth_ind(x, s = 1e6), from = -5, to=5, n = 1001,
      ylab = expression(S(x*";"*s)))
curve(smooth_ind(x, s = 100), col = "blue", add = TRUE, n = 1001)
curve(smooth_ind(x, s = 10), col = "red", add = TRUE, n = 1001)
legend("topright",
       legend = expression("Indicator", S(x*";"*10), S(x*";"*100)),
       fill = c("black", "red", "blue"))
```


<!-- \begin{figure}[hbt!] -->
<!-- \centering -->
<!-- \includegraphics[width=0.5\textwidth]{smoothfun.png} -->
<!-- \caption{Smooth function for indicator function $\mathfrak 1(x\leq 0)$.} -->
<!-- \label{fig:smoothfun} -->
<!-- \end{figure} -->


Specifically, we use
\begin{equation}\label{eq:indsmooth}
  S_{ij}(\tth;s) := S(e_i(\tth)-e_j(\tth);s)
  = \frac{1}{1+\exp(s\cdot(e_i(\tth)-e_j(\tth)))}.
\end{equation}
Notice that as long as $e_i(\tth)$ is a continuous function of $\tth$ for all $i=1,\cdots,n$, the function (\ref{eq:indsmooth}) is indeed a continuous function of $\tth$.

The log smoothed censored EL (log SCEL) is then defined as
\begin{equation}\label{eq:logel_smoo}
  \ell_{\SCEL}(\tth) = \sum_{i=1}^n \Bigl[\delta_i\log(w_i(\tth)) +
    (1-\delta_i)\log(\sum_{j=1}^n S_{ij}(\tth;s)\cdot w_j(\tth))\Bigr].
\end{equation}


### EM algorithm for smoothing

Recall that if $\delta_i=1$, we have $e_i=\e_i$. This means that the \textbf{unobserved (latent) variables} are the $\e_i'$s such that $\delta_i = 0$.

<!-- , the \textbf{complete data likelihood} is -->
<!-- \begin{equation}\label{eq:complike} -->
<!--   \ell(\w,\e|\uu) = \sum_{i=1}^n\log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)}) -->
<!-- \end{equation} -->

<!-- The E-step of the EM algorithm takes the expectation of (\ref{eq:complike}) with respect to $\yy$ (vector of all latent variables), conditioned on the observed values, the censoring indicator and the current state of the parameters. Since $\delta_i = 1$ (meaning $u_i=y_i$), we have -->

<!-- \begin{equation}\label{eq:estep} -->
<!-- \begin{split} -->
<!--   \E_{\e|e,\delta,\w_0}\bigl[\ell(\w,\e|e)\bigr] -->
<!--   &= \E_{\e|e,\delta,\w_0}\Bigl[\sum_{i=1}^n\delta_i\log(\w_i) + -->
<!--   (1-\delta_i)\log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)})\Bigr] \\ -->
<!--   &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) + -->
<!--   (1-\delta_i)\sum_{j=1}^n \E_{\e_i|e,\delta,\w_0}[\mathfrak1(\e_i=e_j)]\log(\w_j)\Bigr] \\ -->
<!--   &= \sum_{i=1}^n\Bigl[\delta_i\log(\w_i) + -->
<!--   (1-\delta_i)\sum_{j=1}^n P_{\e_i|e,\delta,\w_0}(\e_i=e_j)\log(\w_j)\Bigr]. -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Notice that we can write -->
<!-- \[ -->
<!--   \log(\prod_{j=1}^n \w_j^{\mathfrak 1(\e_i=e_j)}) = -->
<!--   \sum_{i=1}^n \mathfrak 1(\e_i=e_j)\log(\w_j), -->
<!-- \] -->
<!-- because for any $i \in \{1,\cdots,n\}$, $\mathfrak 1(\e_i=e_j)=1$ for one and only one $j\in\{1,\cdots,n\}$. Also, the latent $\e_i's$ are independent but not identically distributed, since each of them follows a different categorical distribution (multinomial distribution with one trial). -->

<!-- The conditional distribution in (\ref{eq:condprob_orig}) is a categorical distribution conditioned on that the probability mass is allocated in a subset of $\{e_1,\cdots,e_n\}$ such that $\mathfrak 1(e_j\geq e_i) = 1$ for $j=1,\cdots,n$, which is still a multinomial distribution, -->
<!-- \begin{equation}\label{eq:condprob_orig} -->
<!--   P_{\e_i|e,\delta,\w_0}(\e_i=e_j) = -->
<!--   \frac{\mathfrak 1(e_j\geq e_i)\cdot \w_{0j}}{\sum_{k=1}^n\mathfrak 1(e_k\geq e_i)\cdot\w_{0k}}. -->
<!-- \end{equation} -->

<!-- With smoothing function (\ref{eq:indsmooth}), the probability (\ref{eq:condprob_orig}) can be converted to   -->
<!-- \begin{equation}\label{eq:condprob_smooth} -->
<!--   P_{\e_i|e,\delta,\w_0,s}(\e_i=e_j) = -->
<!--   \frac{ S_{ij}(\tth;s)\cdot \w_{0j}}{\sum_{k=1}^n S_{ik}(\tth;s)\cdot\w_{0k}}. -->
<!-- \end{equation} -->



Therefore, the EM algorithm for smoothing iterates between the following two steps:

\begin{itemize}
\item \textbf{E-step:} Given the observed values and the weights $\w_0$ from the previous iteration, the expectation of the log likelihood is
\begin{equation}\label{eq:emestep}
\begin{split}
  \E_{\e|e,\delta,\w_0,s}[\ell(\w,\e|e)]
  &= \sum_{i=1}^n \Big[\delta_i + \sum_{j = 1}^n (1-\delta_j)
      \cdot\tilde\w_{ji}\Big]\cdot\log \w_i,
\end{split}
\end{equation}
where
\[
  \tilde \w_{ji} = \frac{S_{ij}(\tth;s) \cdot \w_{0i}}{\sum_{k = 1}^n S_{ik}(\tth;s) \cdot \w_{0k}}, \quad j,i: 1,\cdots,n.
\]

\item \textbf{M-step:}
Let $q_i = \delta_i + \sum_{j=1}^n (1-\delta_j)\cdot \tilde \w_{ji}$ for $i=1,\cdots,n$, then the problem becomes
\begin{equation}\label{eq:emmstep}
\begin{split}
  \max_{\w}\quad & \sum_{i=1}^n q_i \log \w_i \\
  \mbox{s.t.}\quad & \sum_{i=1}^n \w_i\cdot \gg(\xx_i,u_i;\tth) = 0 \\
  & \sum_{i=1}^n \w_i = 1 \\
  & \w_i \geq 0, \quad i=1\cdots,n,
\end{split}
\end{equation}

which has the form of the weighted empirical likelihood inner optimization problem, and $\oom = (\w_1,\cdots,\w_n)$ can be updated as given by equations (\ref{eq:omegahat}) to (\ref{eq:optim}).

\end{itemize}

<!-- \begin{itemize} -->
<!-- \item \textbf{E-step:} Given the observed values and the weights $\w_0$ from the previous iteration, the expectation of the log likelihood is -->
<!-- \begin{equation}\label{eq:emestep} -->
<!-- \begin{split} -->
<!--   \E_{\e|e,\delta,\w_0,s}[\ell(\w,\e|e)] -->
<!--   &= \sum_{i=1}^n \Big[\delta_i \log \w_i + -->
<!--     (1-\delta_i) \sum_{j = 1}^n \tilde \w_{ij} \log \w_j\Big] \\ -->
<!--   &= \sum_{i=1}^n \Big[\delta_i + \sum_{j = 1}^n (1-\delta_j) -->
<!--       \cdot\tilde\w_{ji}\Big]\cdot\log \w_i, -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- where -->
<!-- \[ -->
<!--   \tilde \w_{ji} = \frac{S_{ij}(\tth;s) \cdot \w_{0i}}{\sum_{k = 1}^n S_{ik}(\tth;s) \cdot \w_{0k}}, \quad j,i: 1,\cdots,n. -->
<!-- \] -->

<!-- \item \textbf{M-step:} -->
<!-- Let $q_i = \delta_i + \sum_{j=1}^n (1-\delta_j)\cdot \tilde \w_{ji}$ for $i=1,\cdots,n$, then the problem becomes -->
<!-- \begin{equation}\label{eq:emmstep} -->
<!-- \begin{split} -->
<!--   \max_{\w}\quad & \sum_{i=1}^n q_i \log \w_i \\ -->
<!--   \mbox{s.t.}\quad & \sum_{i=1}^n \w_i\cdot \gg(\xx_i,u_i;\tth) = 0 \\ -->
<!--   & \sum_{i=1}^n \w_i = 1 \\ -->
<!--   & \w_i \geq 0, \quad i=1\cdots,n, -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- which has the form of the weighted empirical likelihood inner optimization problem, and $\oom = (\w_1,\cdots,\w_n)$ can be updated as given by equations (\ref{eq:omegahat}) to (\ref{eq:optim}). -->

<!-- \end{itemize} -->

With support correction, we can calculate the weight $r_{n+1}$ (before normalization) for the additional $\log w_{n+1}$ by assigning the censoring indicator as $0$ and the residual as $-\inf$. This minimizes the impact of the fake observation on the other observations.



## Bayesian EL



The following sections will present the Bayesian EL(BayesEL) estimation, as well as an example incorporating \pkg{flexEL} and \proglang{Stan}.

The BayesEL method combines the prior information on parameters and the data-driven likelihood to draw inference on the estimand that is defined by estimating equations. With the prior density $\pi(\tth)$ over the support $\Theta$, the posterior is defined as:

\begin{equation}
\pi(\tth\mid\xx) = \frac{L(\tth\mid\xx)\pi(\tth)}{\int L(\tth\mid\xx)\pi(\tth) \text{d}\tth}
\propto \left[ \prod_{i=1}^{n}w_i \right] \cdot \pi(\tth)
\end{equation}

Many researchers apply the empirical likelihood in a Bayesian framework to obtain favorable results and properties. @rao2010bayesian propose a Bayesian pseudo-empirical-likelihood approach, by using the Bayesian empirical likelihood to survey research. The approach provides asymptotically valid intervals under the design-based set-up and is shown to be flexible in complex survey areas. @yang2012bayesian evaluates the Bayesian empirical likelihood for quantile regression. They demonstrate that the posterior with any fixed prior is asymptotically normal. Also the authors illustrate several common features across quantiles with informative priors. @chaudhuri.ghosh11 takes empirical likelihood into small area estimation. Their proposed method needs not the requirement of parametric likelihood nor the linearity of the estimators. 



There are also many researchers interested in the computation of Bayesian empirical likelihood. @mengersen2013bayesian develop the Bayesian computation with empirical likelihood. The proposed model provides significant time savings compared to the approximate Bayesian computation. @schennach2005bayesian proposes a method called 'Bayesian exponentially tilted empirical likelihood', which enjoys the property of empirical likelihood and exponentially tilting methods.  @chaudhuri-elhmc found that the posterior distribution is usually in a non-parametric manner and its support can be irregular, which confronts the adoption of the Gibbs sampling and the Random Walk Metropolis sampling. Hence they utilized the results from convex analysis and adapted the Hamiltonian Monte Carlo(HMC) sampling, a gradient-based method, to overcome these restraints and solve problems with minimal assumptions.

<!-- As the analytic form of $\pi(\tth\mid\xx)$ is generally absent, the Markov Chain Monte Carlo (MCMC) sampling can be applied to make inference from the posterior distribution. However, the posterior distribution is usually in a non-parametric manner and its support can be irregular, which confronts the adoption of the Gibbs sampling and the Random Walk Metropolis sampling. @chaudhuri-elhmc utilized the results from convex analysis and adapted the Hamiltonian Monte Carlo(HMC) sampling, a gradient-based method, to overcome these restraints and solve problems with minimal assumptions. -->


The HMC has three parameters to be tuned: the mass matrix $M$, the discretization step size $\epsilon$ and the number of steps taken $L$. @Neal2012 gives detailed geometry underlying and discloses the message that the inverse mass matrix corresponds to the variance matrix of the target distribution. In the light of this fact, the mass can be roughly estimated from the inverse variance of the posterior distribution. In practice, it is good to simply set $M$ as an identity matrix. On the other hand, if the mass matrix is poorly estimated, a smaller step size has to be adopted to assure the accuracy of the proposal and, as a result, more leapfrog steps will be taken to ensure the efficiency.

@Matthew2011 points out that the performance of HMC is highly sensitive to the the step size $\epsilon$ and the number of steps $L$. If $L$ is too small, the algorithm exhibits undesirable random walk behavior; yet, a large $L$ will lead to a waste of computation. The No-U-Turn Sampler(NUTS), a variant of HMC, will automatically make an appropriate choice of $L$ in each iteration. Generally speaking, NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. As shown in @Matthew2011, NUTS empirically has an efficient performance as a HMC algorithm that is well tuned.

<!-- \pkg{stanEL} deciphered the fast gradient calculation provided by \pkg{flexEL} and gain computational efficiency as compared to use \proglang{Stan}'s auto differentiation engine. \pkg{stanEL} sources \pkg{flexEL} and wraps the 'logEL' calculation in \proglang{Stan}. -->

\pkg{stanEL} utilizes the custom gradient provided by \pkg{flexEL}, which is more computational efficient and faster than \proglang{Stan}'s auto differentiation engine. \pkg{stanEL} sources \pkg{flexEL} and wraps the 'logEL' calculation in \proglang{Stan}.

# Software Design and Illustrations

The main purpose of \pkg{flexEL} is to support fast and flexible computation of empirical likelihood given estimating equations. The package also supports regression with right-censored responses based on an EM algorithm described in the previous section. In addition, support correction and continuity correction are also included in the package, which are easily turned on and off by the users. 

The package's source code is written as a header-only library in C++ using object-oriented programming. The main class `GenEL` provides the core computations of weighted log empirical likelihood. `CensEL` class utilizes `GenEL` and contains the EM algorithm for right-censored EL computations. The package also provides an \proglang{R} interface mimicking the class structure in C++ based on R6. An R6 object created via the \proglang{R} interface connects to the C++ layer with an external pointer. In this way, fast computation and efficient memory allocation can be achieved. This section describes how a user can interact with the package either in C++ or \proglang{R}.

In terms of built-in models, \pkg{flexEL} provides implementations of estimating equations for mean and quantile regressions based on the following location-scale model:

\begin{equation} \label{md:lsmod}
  y_i = \xx_i'\bbe + \s\cdot\exp(\zz_i'\gga)\cdot\e_i, \quad i=1,\cdots,n,
\end{equation}
where $\xx_i$ and $\zz_i$ are two covariate vectors, $\bbe$ is the location parameter, $\s$ and $\gga$ are two scale parameters, $\e_i$ follows an unknown distribution with mean $0$ and variance $1$. 

as well as the common location model:

\begin{equation} \label{md:lmod}
  y_i = \xx_i'\bbe + \e_i, \quad i=1,\cdots,n.
\end{equation}
where $\e_i$ follows an unknown distribution but only with mean $0$. 


Users can also leverage the EL calculation for customized models specified by estimating equations, in which case, a user can either create a function in \proglang{R} that returns the `G` matrix given the value of the parameters (call it e.g. `evalG`), or implement the calculation in C++ before wrapping into an \proglang{R} interface. 

Several optimization routines are available in \proglang{R}. If analytic gradient of `G` with respect to the parameters can be derived, a user can also provide a corresponding gradient function. Combined with the gradient of log EL with respect to `G` via chain rule, a user can then utilize `optim` or `nlm` in \proglang{R} for parameter estimation with gradient-based methods.

## Mean Regression

In this section, we illustrate how one can interact with the C++ API or \proglang{R} API with the built-in location mean regression model and achieve parameter estimation in \proglang{R}. 

The following script creates a function in C++ that can be exported to \proglang{R} using Rcpp:
```{r, echo = FALSE, results = "asis"}
cat("```c", readLines("mr_neglogel.cpp"), "```", sep = "\n")
```

After sourcing the function into \proglang{R} with Rcpp, one can then use e.g. `nlm` for negative log EL minimization. The following uses simulated data to illustrate this:
```{r, warn=FALSE, include=TRUE, eval=TRUE}
Rcpp::sourceCpp("mr_neglogel.cpp")
beta0 <- c(1,2)
n <- 200
X <- cbind(1, rnorm(n))
eps <- rnorm(n)
y <- c(X %*% beta0 + eps)
# mr_neglogel(c(0.75, 1.25), t(X), y, verbose = TRUE)
nlm(mr_neglogel, c(0.75, 1.25), y, t(X), FALSE)
```

If a user wishes not to use the C++ API directly, they can write a similar function in \proglang{R} as follows:

```{r, echo=TRUE, eval=TRUE}
library(flexEL)
mr_neglogel <- function(beta, y, X, gel) {
  G <- flexEL::mr_evalG(y, X, beta)
  return(-gel$logel(G))
}

gel <- flexEL::GenEL$new(n_obs = n, n_eqs = 2) # initalize an GenEL object
gel$supp_adj <- TRUE # turn on support correction

nlm(mr_neglogel, c(0.75, 1.25), y, X, gel)
```
The usage is similar with right-censored outcomes, e.g. using \proglang{R}:
```{r, echo=TRUE, eval=TRUE}
mr_neglogcel <- function(beta, y_obs, X, cel) {
  G <- flexEL::mr_evalG(y, X, beta)
  -gel$logel(G)
}

cel <- flexEL::CensEL$new(n_obs = n, n_eqs = 2)
cel$supp_adj <- TRUE
cel$smooth <- TRUE # turn on continuity correction
cel$smooth_s <- 1 # set tuning parameter for continuity correction

z <- rnorm(2*mean(y), n = n) # censoring variable
delta <- y <= z # censoring indicators
y_obs <- y
y_obs[!delta] <- z[!delta]

nlm(mr_neglogcel, c(0.75, 1.25), y_obs, X, cel)
```

<!-- - Show users how to create their own `Gfun` and use this from \proglang{R}.   -->

<!-- - Can include gradient-based optimization here as well.  I suggest to use `optim(method = "BFGS")` for optimization, not `nlm()` (it's just more complicated/annoying).  I would say use `nlminb()` which is way faster than `optim()`, but for some reason R Core suggests to use the other optimizers instead... -->

## Quantile Regression

For the location-scale model \eqref{md:lsmod}, the $\tau\times 100\%$ conditional quantile of $y_i$ is
\begin{equation} \label{eq:lsqr}
  Q_{\tau}(y_i|\xx_i) = \xx_i' \bbe + \s\cdot\exp(\zz_i' \gga)\cdot\nu_{\tau}.
\end{equation}

In this case, for parameters $\bbe,\gga$ and $\s$, we adopt the same estimating equation as in the mean regression case. For the quantile parameter $\nu_{\tau}$, we rely on the "check function" introduced by @koenker-bassett1978, which is defined as
\begin{equation}\label{eq:check}
\rho_\tau(u) = u\cdot(\tau - \mathfrak 1 \{u \le 0\}),
\end{equation}
where $\mathfrak 1\{\cdot\}$ is the indicator function.

If the $\tau$-th quantile value of $\e_i\iid(0,1)$ is $\nu_{\tau}$, then $\e_i-\nu_{\tau}$ has $\tau$-th quantile value $0$. The estimator of $\nu_{\tau}$ is then defined as
\begin{equation}\label{eq:nueq}
  \hat\nu_{\tau} = \argmin_{\tilde\nu_{\tau}} \E\Biggl[\rho_{\tau}\Bigl(\frac{y-\xx'\bbe}{\s\cdot\exp(\zz'\g)}-\tilde\nu_{\tau}\Bigr)\Biggr].
\end{equation}

As before, we use the first order optimality condition of (\ref{eq:nueq}) to obtain the estimating equation for $\nu_{\tau}$. Therefore, we obtain all the moment conditions for quantile regression as follows
\begin{equation} \label{eq:qrls}
\begin{split}
  \E\Bigl[\frac{y-\xx'\bbe}{\exp(2\zz'\gga)}\cdot\xx\Bigr] &= 0 \\
  \E\Bigl[\bigl(1-\frac{(y-\xx'\bbe)^2}{\s^2\cdot\exp(2\zz'\gga)}\bigr)\cdot\zz\Bigr] &= 0 \\
  \E\Bigl[\frac{(y-\xx'\bbe)^2}{\s^2\cdot\exp(2\zz'\gga)}-1\Bigr] &= 0 \\
  \E\Bigl[\rho'_{\tau}\bigl(\frac{y-\xx'\bbe}{\s\cdot\exp(\zz'\gga)}-\nu_{\tau}\bigr)\Bigr] &= 0.
\end{split}
\end{equation}

Notice that the indicator function used by the "check function" is also a source of discontinuity in log EL even in the absence of censoring. Although there are other approaches to smooth out the discontinuity in quantile regression, such as @chen2007, splines or kernel methods, the same trick used for correcting the discontinuity caused by right-censoring is particularly straightforward to be applied here as well. That is, we modify \eqref{eq:check} as follows:

\begin{equation}
\rho_{S,\tau}(u;s) = u\cdot(\tau - S(u;s)),
\end{equation}
where $S$ is defined as in \eqref{eq:sigmoid}.

Usage of quantile regression in \pkg{flexEL} is very similar to mean regression in the last section. Moreover, one can simultaneously estimte parameters at multiple quantile levels both with the location and location-scale models. With the location model, both the intercept and the slope parameters can be different at different quantile levels. Essentially, the `G` matrix is obtained by stacking together multiple `G` matrices each at one quantile level. This is one advantage of quantile regression when heteroscedasticity presents. With the location-scale model, only the quantile parameter $\nu_{\tau}$ is different at different quantile levels, since heteroscedasticity can be handled by the scale function.

## User-defined Model

The following toy example illustrates how to use \pkg{flexEL} with a customized model. Consider a simple two-parameter linear regression model
\[
  y_i = \beta_0 + \beta_1x_i + \e_i, \quad i=1,\cdots,n, 
\]
where $\epsilon_i \iid \text{F}(\epsilon)$ such that $\E(\epsilon_i) = 0$ and $\text{Var}(\epsilon_i) = 1$.

As the first step, one should design the estimating equations of the parameters and implement a function (`evalG`) that returns the corresponding $G$ matrix of dimension $n\times m$, where $n$ is the number of observations, and $m$ is the dimension of the range of the estimating equation. 

In this example, the following estimating equation can be used, which has a range of dimension $m=p$, same as the dimension of the parameter vector $\beta$:
\[
  \E[(y-x'\beta)\cdot x] = 0,
\]
which is
\[
  \sum_{i=1}^n w_i\cdot (y_i-x_i'\beta)\cdot x_i = 0.
\]
In other words, the $g$ function is specified as
\[
  g(x,y;\beta) = (y-x'\beta)\cdot x.
\]

This can be implemented in R (called `mr2_evalG`) as follows: 
```{r, echo=TRUE, eval=TRUE}
mr2_evalG <- function(y, X, beta) {
  tX <- t(X)
  yXb <- y - c(X %*% beta)
  G <- sweep(tX, MARGIN = 2, yXb, `*`)
  return(t(G))
}
```

With \pkg{flexEL}, the negative log EL can be implemented as:
```{r, echo=TRUE, eval=TRUE}
mr2_neglogel <- function(beta, y, X, gel) {
  G <- mr2_evalG(y, X, beta)
  return(-gel$logel(G))
}
```

One may also provide the gradient of `mr2_evalG` with respect to the parameters, in this case, it is
```{r, echo=TRUE, eval=TRUE}
mr2_dGdb <- function(y, X, beta) {
  lx <- split(X, row(X))
  dg <- lapply(lx, function(x) -tcrossprod(x,x))
  return(dg)
}
```

Notice that here the gradient is a 3-dimensional matrix. This can then be combined with the gradient of log EL w.r.t `G` to obtain the gradient of log EL w.r.t. the parameters, $\beta$:
```{r, echo=TRUE, eval=TRUE}
mr2_neglogel_grad <- function(beta, y, X, gel) {
  G <- mr2_evalG(y, X, beta)
  logel_lst <- gel$logel_grad(G)
  dldG <- -logel_lst$grad
  dGdb <- mr2_dGdb(y, X, beta)
  grad_mat <- matrix(NA, nrow = nrow(dldG), ncol = ncol(dldG))
  for (ii in 1:nrow(dldG)) {
    grad_mat[ii,] <- dGdb[[ii]] %*% dldG[ii,]
  }
  colSums(grad_mat)
}
```

Then one can use e.g. `optim` to obtain the estimate of $\beta$:
```{r, echo=TRUE, eval=TRUE}
library(flexEL)

# simulate some data
n <- 200
X <- cbind(1, rnorm(n))
beta0 <- c(1, 2)
eps <- (rchisq(n, df=5)-5)/sqrt(2*5)
y <- c(X %*% beta0) + eps

gel <- GenEL$new(n_obs = n, n_eqs = 2)
gel$supp_adj <- TRUE

optim(c(0.75, 1.25), fn = mr2_neglogel, gr = mr2_neglogel_grad,
      y, X, gel, method = "BFGS")
```

Alternatively, a user can implement the `G` matrix and gradient calculations in C++ and follows the example in the mean regression section where instead of including `flexEL/mean_reg_model.h`, include the user defined C++ header file.

<!-- ## Bayesian EL -->

<!-- - Good to illustrate with \proglang{Stan} NUTS algorithm, because we've implemented it already and \proglang{Stan} is a really good autodiff engine + best NUTS implementation. -->

<!-- - Can illustrate with the example from the JRSSB EL-HMC paper. -->

## HMC example with US job satisfaction

As an application of above methodology, we reproduce the example in @chaudhuri-elhmc of a job satisfaction survey of a large corporation in the United States that was previously reported in @fowlkes-1988. The aim of the survey was to relate the job satisfaction to demographic variables and geographical location. The geographical location corresponds to seven regions in the United States: Northeast, Mid-Atlantic, Southern, Midwest, Northwest, Southwest, and Pacific, which are represented by the area-level random effects $z_i$, $i = 1,\ldots, 7$.  The three demographic variables are age (less than 35, 35â€“44 and greater than 44 years), sex (male and female) and race (white and others). As a result, there are $3\times2\times2=12$ combinations (or cells) of these demographic variables for each area/region $i$, such that $N_i \equiv 12$.  A preview of the dataset is displayed below.

```{r datadisp, warning=FALSE, message=FALSE}
require(flexEL)
require(rstan)
require(bayesplot)
require(stanEL)
require(tidyr)
require(dplyr)
require(numDeriv)
require(optimCheck)
options(mc.cores = parallel::detectCores())


head(jobsatis)
```

Let $y_{ij}$ denote the number of employees who were satisfied with their jobs in cell $j$ and area $i$ out of a total $n_{ij}$; then, $\rho_{ij} = E[y_{ij}/n_{ij}]$ measures the job satisfaction. With the above demonstration, the job satisfaction model can be established as follows:
$$
\begin{aligned}
\logit \rho_{ij} & = \xx_{ij}'\bbe + z_i, \\
\xx_{ij}'\bbe & = \mu + \alpha_a + \gamma_s+\eta_r+(\gamma\eta)_{sr}
\end{aligned}
$$
where $\alpha_a, \gamma_s, \eta_r, (\gamma\eta)_{sr}$ denotes the main effects of $a$th age, $s$th sex and $r$th race and first-order interaction effects of sex and race. As a result, there are $1+(3-1) + (2-1) +(2-1)+(2-1)\times(2-1)=6$ parameters to estimate, apart from the seven area-level random effects with $\tau$ being their unknown variance.

### Model and Mshioment Conditions


The following model for small area estimation is presented by @chaudhuri.ghosh11.  Let $y_{ij}$ denote the number of successes for observation $j$ in area $i$ out of $n_{ij}$ trials, and let $\rho_{ij} = E[y_{ij}/n_{ij}]$.  Let $\xx_{ij}$ denote the corresponding covariate vector, and $z_i$ be an area-level random effect, such that
$$
\logit \rho_{ij} = \xx_{ij}'\bbe + z_i, \qquad z_i \iid \N(0, \tau).
$$
Rather than assume the $y_{ij}$ are iid Bernoulli trials with success probability $\rho_{ij}$, Chaudhuri and Gosh simply assume the Bartlett moment conditions
$$
E[y_{ij} - n_{ij} \rho_{ij} \mid z_i] = 0, \qquad E\left[\frac{(y_{ij} - n_{ij} \rho_{ij})^2}{n_{ij}\rho_{ij}(1-\rho_{ij})} - 1 \mid z_i \right] = 0.
$$

### Empirical Likelihood

Let $N_i$ denote the number of observations in area $i$ and $N_A$, the number of areas, such that the total number of observations is $N = \sum_{i=1}^{N_A} N_i$.  Let $\yy_i = (y_{i1},...,y_{iN_{i}})$, and similarly for $\rrh_i$, $\XX_i$, etc.  The empirical likelihood in terms of $\bbe$ and $\zz = (z_1,..., z_{N_A})$ satisfies the sample moments
$$
\sum_{j=1}^{N_i} \omega_{ij} (y_{ij} - n_{ij} \rho_{ij}) = 0, \qquad \sum_{j=1}^{N_i} \omega_{ij} \left(\frac{(y_{ij} - n_{ij} \rho_{ij})^2}{n_{ij}\rho_{ij}(1-\rho_{ij})} - 1\right) = 0, \qquad i = 1,\ldots,N_A,
$$
where $\omega_{ij} > 0$ are the EL weights. If we consider the $y_{ij}$ as coming from a conditional distribution given $z_i$, then we set $\sum_{j=1}^{N_i} \omega_{ij} = 1$ for each $i = 1,\ldots,N_A$.  Thus, we obtain $N_A$ matrices

$$\bm{G}(\bbe, z_i) = [\mmu_i, \mmu_i^2/\nnu_i-1]$$

of size $N_i \times 2$, where $\mmu_i = \yy_i - \bm{n}_i \rrh_i$, $\nnu_i = \bm{n}_i\rrh_i(1-\rrh_i)$, and $\rrh_i = \ilogit(\XX_i \bbe + z_i)$.


If $\logel(\bbe, z_i)$ denotes the log-EL function for each area $i$, then the combined log-EL function for all $N_A$ areas is
$$
\logel(\bbe, \zz) = \sum_{i=1}^{N_A} \logel(\bbe, z_i).
$$


### Parameter Prior

The primary purpose of **stanEL** is to perform Bayesian inference on empirical likelihood by using Stan. For this purpose we must specify a prior on $\beta$ and $\tau$, which we take from @chaudhuri-elhmc :

$$
\begin{aligned}
\bbe  \sim \N(0, \sigma^2),\qquad \tau  \sim \text{IG}(\alpha, \eta),
\end{aligned}
$$

where $\sigma = 10$ and $\alpha = \eta = .1$.

```{r data}

# inverse logit function
ilogit <- function(x) 1/(1+exp(-x))
# density of the inverse gamma distribution
dinvgamma <- function(x, shape, scale, log = FALSE) {
  if(all(shape > 0 & scale > 0)) {
    cst <- shape * log(scale) - lgamma(shape)
  } else cst <- 0
  ld <- cst - (shape + 1) * log(x) - scale/x
  if(!log) ld <- exp(ld)
  ld
}

# prepare the jobsatis dataset for Stan

# convert to tibble and separate satis into satis_Yes and satis_No
jobsatis <- jobsatis %>%
  as_tibble() %>%
  pivot_wider(names_from = "satis", names_prefix = "satis_",
              values_from = "count")

# tabulate satis_Yes counts in each region
tab_yes <- jobsatis %>%
  select(-satis_No) %>%
  pivot_wider(names_from = "region", values_from = "satis_Yes")

# tabulate satis_Tot = (satis_Yes + satis_No) counts in each region
tab_total <- jobsatis %>%
  mutate(satis_Total = satis_Yes + satis_No) %>%
  select(-satis_Yes, -satis_No) %>%
  pivot_wider(names_from = "region", values_from = "satis_Total")

# number of regions
n_area <- nlevels(jobsatis$region)
# number of observations per region
n_obs <- nrow(tab_yes)

# n_area x n_obs matrix of counts of satisfied employees
y <- tab_yes %>%
  select(-race, -age, -gender) %>%
  as.matrix() %>% t()

# n_area x n_obs matrix of total number of employees
N <- tab_total %>%
  select(-race, -age, -gender) %>%
  as.matrix() %>% t()

# common covariate matrix
X <- model.matrix(~ age + gender + race + gender:race, data = tab_yes)

# number of covariates
n_cov <- ncol(X)
```


### MAP Using flexEL

Let us start by finding the maximum *a posteriori* (MAP) estimates of $\bbe$ and  $\zz$ using the [**flexEL**](https://github.com/mlysy/flexEL) package [@R-flexEL].  Note that, instead of writing the log-posterior in terms of $\bbe$ and $\zz$, we instead parametrize it in terms of $\bbe_{\text{raw}} = \bbe/\sigma$ and $\zz_{\text{raw}} = \zz/\tau$, as this typically improves convergence of Stan's MCMC sampler.

```{r flex function}
#' Calculate the `G` matrices for the logistic regression / small area model with common covariate matrix.
#'
#' @param beta Vector of `n_cov` regression parameters.
#' @param z Vector of `n_area` regional parameters.
#' @param y Matrix of `n_area x n_obs` successes.
#' @param N Matrix of `n_area x n_obs` total counts.
#' @param X Common covariate matrix of size `n_obs x n_cov`.
#' @return A list of `n_area` matrices of size `n_obs x 2`.
logisa_G <- function(beta, z, y, N, X) {
  n_cov <- ncol(X)
  n_obs <- nrow(X)
  n_area <- nrow(y)
  G_all <- list()
  for (ii in 1:n_area){
    G <- matrix(NA, nrow = nrow(X), ncol = 2)
    rho <- ilogit(X %*% beta + z[ii])
    nu <- N[ii,] * rho
    mu <- y[ii,] - nu
    nu <- nu*(1 - rho)
    G[,1] <- mu
    G[,2] <- (mu)^2 / nu - 1
    G_all[[ii]] <- G
  }
  return(G_all)
}

#' Calculate the log-EL likelihood for an arbitrary `G` matrix.
#'
#' @param G Matrix of size `n_obs x n_eq`.
# logel <- function(G, max_iter = 10, rel_tol = 1e-3, supp_adj = FALSE,
#                   return_dldG = FALSE) {
#   el <- flexEL::GenEL$new(nrow(G), ncol(G))
#   el$set_opts(max_iter = max_iter, rel_tol = rel_tol, supp_adj = supp_adj)
#   if(return_dldG == T){
#     return(el$logel_grad(G))
#   } else {
#     return(el$logel(G))
#   }
# }
logel <- function(G, max_iter = 10, rel_tol = 1e-3, supp_adj = FALSE,
                  grad = FALSE) {
  ans <- flexEL::logEL(G = G, max_iter = max_iter, rel_tol = rel_tol,
                       supp_adj = supp_adj, grad = grad)
  if(grad && all(is.na(ans$grad))) {
    ans$grad[] <- 0
  }
  ans
}

#' log-EL posterior for logistic regression small area estimation.
#'
#' @param theta Vector of `n_area + n_obs + 1`
#' @param y Matrix of `n_area x n_obs` successes.
#' @param N Matrix of `n_area x n_obs` total counts.
#' @param X Common covariate matrix of size `n_obs x n_cov`.
#' @param beta_sd A positive scalar, standard deviation of beta
#' @param tau_prior Vector of 2, inverse gamma distribution parameter of tau
#' @return Scalar, the log-EL posterior
logisa_G_logel_test <- function(beta_raw, z_raw, tau, y, N, X, beta_sd, tau_prior,
                                supp_adj, max_iter, rel_tol) {
  beta <- beta_raw * beta_sd
  z <- z_raw * sqrt(tau)
  G_all <- logisa_G(beta, z, y, N, X)
  ll <- sapply(G_all, logel, supp_adj = supp_adj, max_iter = max_iter, rel_tol = rel_tol)
  lpi_z <- dnorm(z_raw, log = TRUE)
  lpi_beta <- dnorm(beta_raw, log = TRUE)
  lpi_tau <- dinvgamma(tau, shape = tau_prior[1], scale = tau_prior[2], log = TRUE)
  return(sum(ll) + sum(lpi_z) + sum(lpi_beta) + lpi_tau)
}

#' log-EL posterior for logistic regression small area estimation
#'    with all parameters are embedded into theta
logisa_G_logel_test_theta <- function(theta, y, N, X, beta_sd, tau_prior,
                                supp_adj, max_iter, rel_tol) {
  n_cov <- ncol(X)
  n_obs <- nrow(X)
  n_area <- nrow(y)
  beta_raw <- theta[1:n_cov]
  z_raw <- theta[n_cov + 1:n_area]
  tau <- theta[n_cov + n_area + 1]
  return(logisa_G_logel_test(beta_raw, z_raw, tau, y, N, X, beta_sd, tau_prior,
                             supp_adj, max_iter, rel_tol))
}
```


For this we use the gradient-free Nelder-Mead method provided by `stats::optim()`.



```{r flexELresult}
# initial value
set.seed(8888)
p_init <- c(rnorm(n_cov + n_area, sd = 0.1), runif(1))
beta_sd <- 10
tau_prior <- c(.1, .1)
nlogpost <- function(theta) {
  -logisa_G_logel_test_theta(theta, y = y, N = N, X = X,
                       beta_sd = beta_sd, tau_prior = tau_prior,
                       supp_adj = FALSE, max_iter = 100, rel_tol = 1e-7)
}
optimout <- optim(p_init, fn = nlogpost,
                  control = list(maxit = 1e5, reltol = 1e-10))
optimout$convergence

# Convergence of 0 indicates successful completion;
# Then, estimation is able to be obtained by:
theta_est<- optimout$par
beta_raw_est <- theta_est[1:n_cov]
z_raw_est <- theta_est[n_cov + 1:n_area]
tau_est <- exp(theta_est[n_cov + n_area + 1])
beta_est <- beta_sd * beta_raw_est
z_est <-  sqrt(tau_est) * z_raw_est
Xb_est <- X %*% beta_est
flexELest <- t(ilogit(Xb_est  + z_est[n_area]))
flexELest
```

### MCMC using stanEL

To vectorize the calculation, we set the G as `2 X n_obs`, and assign value by row. Thus, in calling the `logEL_opt`, we do not need to get transpose of G matrix.  The implementation of the Bayesian EL model is provided below.

```{r, echo = FALSE, results = "asis"}
cat("```stan",
    readLines(system.file("stan", "logisa_el.stan",
                          package = "stanEL")),
    "```", sep = "\n")
```

We now turn to MCMC sampling with **rstan**:

```{r stanEL, fig.width=7, fig.height= 4, message=FALSE, warning=F}
beta_sd <- 10
tau_prior <- c(.1,.1)
data <- list(n_area = n_area, n_obs = n_obs, n_cov = n_cov,
             y = y, X = X, N = N,
             beta_sd = beta_sd, tau_prior = tau_prior,
             max_iter = 100, rel_tol = 1e-7, supp_adj= 0)


init_fun <- function() {
  list(beta_raw = beta_raw_est,
       z_raw = z_raw_est,
       tau = tau_est)
}

options(mc.cores = parallel::detectCores())
job_satis_fit <- rstan::sampling(stanEL:::stanmodels$logisa_el,data = data,
                                 chain = 4, init = init_fun)

rstan::traceplot(job_satis_fit,pars = 'rho_Pacific')
stanELest <- summary(job_satis_fit,pars = 'rho_Pacific')[[1]][,1]
```

The decent traceplots justify the convergence, with which the estimation is convincing.

### Comparison of Estimation Methods


Here we compare point estimates of the proportion of satisfied employees in the Pacific region of the USA using several estimators:

- `glm`: The generalized linear model estimate using logistic regression.

- `MAP`: The Bayesian MAP estimate using **flexEL**.

- `NUTS`: The posterior mean of the full Bayesian estimate using **stanEL**.

- `CMY17`: The posterior HMC estimates reported by @chaudhuri-elhmc.

```{r comp}
# glm estimation with logistic regression
glm_est <- glm(cbind(y[n_area,], N[n_area,]-y[n_area,]) ~ X - 1,
               family = "binomial")
glm_est <- predict(glm_est, type = "response")
bayesEL_Chaudhuri <- c(0.636,0.628,.677,.669,.733,.726,.674,.583,.713,.626,.764,.686)

rbind(glm = glm_est, MAP = flexELest, HMC = stanELest,CMY17 = bayesEL_Chaudhuri)
```
CMY17 is the estimator for the Pacific region in the @chaudhuri-elhmc.

The table shows that all three methods present similar results as compared to what @chaudhuri-elhmc had.


# Conclusion


\bibliography{references}
